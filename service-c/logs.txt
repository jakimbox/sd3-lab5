
==> Audit <==
|--------------|-----------------------------|----------|-------------------------|---------|---------------------|---------------------|
|   Command    |            Args             | Profile  |          User           | Version |     Start Time      |      End Time       |
|--------------|-----------------------------|----------|-------------------------|---------|---------------------|---------------------|
| addons       | enable metrics-server       | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 16:54 -05 |                     |
| dashboard    |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 17:20 -05 |                     |
| ip           |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 17:21 -05 |                     |
| dashboard    |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 17:27 -05 |                     |
| dashboard    | --url                       | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 17:28 -05 |                     |
| service      |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:12 -05 |                     |
| service      | name: predictapp-deployment | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:13 -05 |                     |
| service      | name: predictapp-deployment | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:13 -05 |                     |
| service      |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:17 -05 |                     |
| service      | predictapp-deployment --url | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:18 -05 |                     |
| dashboard    |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:20 -05 |                     |
| ip           | node                        | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:20 -05 |                     |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:31 -05 | 10 Sep 24 18:31 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:33 -05 | 10 Sep 24 18:33 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:37 -05 | 10 Sep 24 18:37 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 10 Sep 24 18:38 -05 | 10 Sep 24 18:38 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 12:34 -05 | 11 Sep 24 12:34 -05 |
| delete       |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 12:36 -05 | 11 Sep 24 12:36 -05 |
| delete       | --all                       | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 12:44 -05 | 11 Sep 24 12:44 -05 |
| start        | --nodes 2 -p nodes          | nodes    | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 13:30 -05 | 11 Sep 24 13:34 -05 |
| dashboard    |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 13:34 -05 |                     |
| start        |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 13:35 -05 | 11 Sep 24 13:38 -05 |
| dashboard    |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 13:38 -05 |                     |
| ip           |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 13:56 -05 | 11 Sep 24 13:56 -05 |
| service      | predictapp-service          | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 13:58 -05 | 11 Sep 24 13:59 -05 |
| dashboard    |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:00 -05 |                     |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:05 -05 | 11 Sep 24 14:05 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:07 -05 | 11 Sep 24 14:07 -05 |
| start        | --nodes 2 -p nodes          | nodes    | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:07 -05 |                     |
| node         | add                         | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:08 -05 | 11 Sep 24 14:11 -05 |
| service      | predictapp-service          | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:15 -05 | 11 Sep 24 14:16 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:30 -05 | 11 Sep 24 14:30 -05 |
| node         | add                         | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:38 -05 | 11 Sep 24 14:43 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 14:40 -05 | 11 Sep 24 14:40 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 16:11 -05 | 11 Sep 24 16:11 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 16:12 -05 | 11 Sep 24 16:12 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 16:16 -05 | 11 Sep 24 16:16 -05 |
| service      | predictapp-service          | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 16:17 -05 | 11 Sep 24 16:18 -05 |
| service      | predictapp-service          | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 18:06 -05 | 11 Sep 24 18:17 -05 |
| service      | predictapp-service          | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 11 Sep 24 18:17 -05 |                     |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 18 Sep 24 19:31 -05 | 18 Sep 24 19:31 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 23 Sep 24 19:29 -05 | 23 Sep 24 19:29 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 27 Sep 24 23:59 -05 | 27 Sep 24 23:59 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 18 Oct 24 17:12 -05 | 18 Oct 24 17:12 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 19 Oct 24 00:33 -05 | 19 Oct 24 00:33 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 18:27 -05 | 22 Oct 24 18:27 -05 |
| start        |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 18:59 -05 | 22 Oct 24 19:05 -05 |
| ip           |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 19:16 -05 | 22 Oct 24 19:16 -05 |
| service      | service-c                   | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 19:21 -05 | 22 Oct 24 19:23 -05 |
| service      | service-c                   | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 19:39 -05 | 22 Oct 24 19:39 -05 |
| service      | service-service-c           | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 20:00 -05 |                     |
| service      | service-service             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 20:01 -05 | 22 Oct 24 20:06 -05 |
| service      | service-service             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 20:08 -05 | 22 Oct 24 20:16 -05 |
| service      | service-service             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 20:16 -05 | 22 Oct 24 20:16 -05 |
| service      | service-service             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 20:29 -05 | 22 Oct 24 20:29 -05 |
| service      | service-service             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 20:34 -05 | 22 Oct 24 20:38 -05 |
| start        |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 21:07 -05 | 22 Oct 24 21:27 -05 |
| update-check |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 22:54 -05 | 22 Oct 24 22:54 -05 |
| start        |                             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 23:12 -05 | 22 Oct 24 23:14 -05 |
| service      | service-service             | minikube | DESKTOP-89GVS0I\Usuario | v1.34.0 | 22 Oct 24 23:28 -05 |                     |
|--------------|-----------------------------|----------|-------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/10/22 23:12:00
Running on machine: DESKTOP-89GVS0I
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1022 23:12:00.714910   10064 out.go:345] Setting OutFile to fd 84 ...
I1022 23:12:00.797089   10064 out.go:397] isatty.IsTerminal(84) = true
I1022 23:12:00.797089   10064 out.go:358] Setting ErrFile to fd 88...
I1022 23:12:00.797089   10064 out.go:397] isatty.IsTerminal(88) = true
I1022 23:12:00.876701   10064 out.go:352] Setting JSON to false
I1022 23:12:00.882417   10064 start.go:129] hostinfo: {"hostname":"DESKTOP-89GVS0I","uptime":2388,"bootTime":1729654332,"procs":238,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.4894 Build 19045.4894","kernelVersion":"10.0.19045.4894 Build 19045.4894","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"701f1f05-191b-4133-8271-71e8c8aefa90"}
W1022 23:12:00.882953   10064 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1022 23:12:00.885655   10064 out.go:177] 😄  minikube v1.34.0 on Microsoft Windows 10 Pro 10.0.19045.4894 Build 19045.4894
I1022 23:12:00.889513   10064 notify.go:220] Checking for updates...
I1022 23:12:00.926174   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:12:00.930367   10064 driver.go:394] Setting default libvirt URI to qemu:///system
I1022 23:12:01.215123   10064 docker.go:123] docker version: linux-27.1.1:Docker Desktop 4.33.1 (161083)
I1022 23:12:01.267406   10064 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1022 23:12:03.780882   10064 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.5134767s)
I1022 23:12:03.785795   10064 info.go:266] docker info: {ID:c0ec982f-4882-4534-b117-fadb5f4ad00a Containers:70 ContainersRunning:38 ContainersPaused:0 ContainersStopped:32 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:265 OomKillDisable:true NGoroutines:264 SystemTime:2024-10-23 04:12:03.754859392 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4028239872 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I1022 23:12:03.787104   10064 out.go:177] ✨  Using the docker driver based on existing profile
I1022 23:12:03.788805   10064 start.go:297] selected driver: docker
I1022 23:12:03.789360   10064 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime: ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime: ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Usuario:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1022 23:12:03.789913   10064 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1022 23:12:03.862760   10064 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1022 23:12:04.584375   10064 info.go:266] docker info: {ID:c0ec982f-4882-4534-b117-fadb5f4ad00a Containers:70 ContainersRunning:38 ContainersPaused:0 ContainersStopped:32 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:265 OomKillDisable:true NGoroutines:264 SystemTime:2024-10-23 04:12:04.556827543 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4028239872 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I1022 23:12:04.809288   10064 cni.go:84] Creating CNI manager for ""
I1022 23:12:04.809288   10064 cni.go:136] multinode detected (3 nodes found), recommending kindnet
I1022 23:12:04.809822   10064 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Usuario:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1022 23:12:04.810967   10064 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1022 23:12:04.812604   10064 cache.go:121] Beginning downloading kic base image for docker with docker
I1022 23:12:04.813144   10064 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1022 23:12:04.814778   10064 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1022 23:12:04.814778   10064 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1022 23:12:04.816404   10064 preload.go:146] Found local preload: C:\Users\Usuario\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1022 23:12:04.816404   10064 cache.go:56] Caching tarball of preloaded images
I1022 23:12:04.816948   10064 preload.go:172] Found C:\Users\Usuario\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1022 23:12:04.816948   10064 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1022 23:12:04.817491   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
W1022 23:12:05.007672   10064 image.go:95] image docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1022 23:12:05.007672   10064 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1022 23:12:05.008870   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:12:05.008870   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:12:05.009420   10064 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1022 23:12:05.011599   10064 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1022 23:12:05.011599   10064 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1022 23:12:05.012152   10064 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1022 23:12:05.012152   10064 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1022 23:12:05.012152   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:12:08.249323   10064 cache.go:164] successfully loaded and using docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1022 23:12:08.249895   10064 cache.go:194] Successfully downloaded all kic artifacts
I1022 23:12:08.252151   10064 start.go:360] acquireMachinesLock for minikube: {Name:mk355840099b61e2912d443a688a50890ecbf0b8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1022 23:12:08.252783   10064 start.go:364] duration metric: took 631.1µs to acquireMachinesLock for "minikube"
I1022 23:12:08.252783   10064 start.go:96] Skipping create...Using existing machine configuration
I1022 23:12:08.252783   10064 fix.go:54] fixHost starting: 
I1022 23:12:08.325431   10064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:12:08.473988   10064 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1022 23:12:08.473988   10064 fix.go:138] unexpected machine state, will restart: <nil>
I1022 23:12:08.475893   10064 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1022 23:12:08.517854   10064 cli_runner.go:164] Run: docker start minikube
I1022 23:12:10.309981   10064 cli_runner.go:217] Completed: docker start minikube: (1.7921277s)
I1022 23:12:10.339350   10064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:12:10.495716   10064 kic.go:430] container "minikube" state is running.
I1022 23:12:10.523151   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1022 23:12:10.679685   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
I1022 23:12:10.685306   10064 machine.go:93] provisionDockerMachine start ...
I1022 23:12:10.717118   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:10.906750   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:12:10.933695   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51539 <nil> <nil>}
I1022 23:12:10.933695   10064 main.go:141] libmachine: About to run SSH command:
hostname
I1022 23:12:10.940803   10064 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1022 23:12:13.953797   10064 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1022 23:12:17.306545   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1022 23:12:17.308430   10064 ubuntu.go:169] provisioning hostname "minikube"
I1022 23:12:17.331475   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:17.461119   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:12:17.461879   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51539 <nil> <nil>}
I1022 23:12:17.461879   10064 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1022 23:12:17.878205   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1022 23:12:17.903796   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:18.028702   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:12:18.029251   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51539 <nil> <nil>}
I1022 23:12:18.029251   10064 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1022 23:12:18.349322   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1022 23:12:18.349322   10064 ubuntu.go:175] set auth options {CertDir:C:\Users\Usuario\.minikube CaCertPath:C:\Users\Usuario\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Usuario\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Usuario\.minikube\machines\server.pem ServerKeyPath:C:\Users\Usuario\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Usuario\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Usuario\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Usuario\.minikube}
I1022 23:12:18.349894   10064 ubuntu.go:177] setting up certificates
I1022 23:12:18.349894   10064 provision.go:84] configureAuth start
I1022 23:12:18.372220   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1022 23:12:18.491786   10064 provision.go:143] copyHostCerts
I1022 23:12:18.516351   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/cert.pem, removing ...
I1022 23:12:18.516907   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\cert.pem
I1022 23:12:18.517449   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\cert.pem --> C:\Users\Usuario\.minikube/cert.pem (1123 bytes)
I1022 23:12:18.542074   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/key.pem, removing ...
I1022 23:12:18.542074   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\key.pem
I1022 23:12:18.543613   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\key.pem --> C:\Users\Usuario\.minikube/key.pem (1679 bytes)
I1022 23:12:18.568915   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/ca.pem, removing ...
I1022 23:12:18.568915   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\ca.pem
I1022 23:12:18.570101   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\ca.pem --> C:\Users\Usuario\.minikube/ca.pem (1082 bytes)
I1022 23:12:18.571930   10064 provision.go:117] generating server cert: C:\Users\Usuario\.minikube\machines\server.pem ca-key=C:\Users\Usuario\.minikube\certs\ca.pem private-key=C:\Users\Usuario\.minikube\certs\ca-key.pem org=Usuario.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I1022 23:12:18.984637   10064 provision.go:177] copyRemoteCerts
I1022 23:12:19.041702   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1022 23:12:19.057767   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:19.183982   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51539 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube\id_rsa Username:docker}
I1022 23:12:19.393987   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1022 23:12:19.516227   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1082 bytes)
I1022 23:12:19.630651   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I1022 23:12:19.770555   10064 provision.go:87] duration metric: took 1.4200076s to configureAuth
I1022 23:12:19.770555   10064 ubuntu.go:193] setting minikube options for container-runtime
I1022 23:12:19.772442   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:12:19.798819   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:19.952078   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:12:19.952743   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51539 <nil> <nil>}
I1022 23:12:19.952792   10064 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1022 23:12:20.490765   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1022 23:12:20.490765   10064 ubuntu.go:71] root file system type: overlay
I1022 23:12:20.492036   10064 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1022 23:12:20.515282   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:20.651085   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:12:20.652478   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51539 <nil> <nil>}
I1022 23:12:20.653043   10064 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1022 23:12:21.029438   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1022 23:12:21.053463   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:21.271133   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:12:21.271752   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51539 <nil> <nil>}
I1022 23:12:21.272281   10064 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1022 23:12:21.604410   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1022 23:12:21.604410   10064 machine.go:96] duration metric: took 10.9191039s to provisionDockerMachine
I1022 23:12:21.604991   10064 start.go:293] postStartSetup for "minikube" (driver="docker")
I1022 23:12:21.604991   10064 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1022 23:12:21.654456   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1022 23:12:21.679441   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:21.840287   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51539 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube\id_rsa Username:docker}
I1022 23:12:22.181636   10064 ssh_runner.go:195] Run: cat /etc/os-release
I1022 23:12:22.194914   10064 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1022 23:12:22.194914   10064 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1022 23:12:22.194914   10064 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1022 23:12:22.194914   10064 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1022 23:12:22.196055   10064 filesync.go:126] Scanning C:\Users\Usuario\.minikube\addons for local assets ...
I1022 23:12:22.197193   10064 filesync.go:126] Scanning C:\Users\Usuario\.minikube\files for local assets ...
I1022 23:12:22.197743   10064 start.go:296] duration metric: took 592.7525ms for postStartSetup
I1022 23:12:22.247891   10064 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1022 23:12:22.268715   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:22.394996   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51539 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube\id_rsa Username:docker}
I1022 23:12:22.641863   10064 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1022 23:12:22.662128   10064 fix.go:56] duration metric: took 14.4093456s for fixHost
I1022 23:12:22.662128   10064 start.go:83] releasing machines lock for "minikube", held for 14.4093456s
I1022 23:12:22.683090   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1022 23:12:22.816119   10064 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1022 23:12:22.842500   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:22.858387   10064 ssh_runner.go:195] Run: cat /version.json
I1022 23:12:22.880819   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:12:22.996955   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51539 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube\id_rsa Username:docker}
I1022 23:12:23.025296   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51539 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube\id_rsa Username:docker}
W1022 23:12:23.219524   10064 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1022 23:12:23.344753   10064 ssh_runner.go:195] Run: systemctl --version
I1022 23:12:23.459971   10064 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1022 23:12:23.554504   10064 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1022 23:12:23.611584   10064 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1022 23:12:23.659090   10064 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1022 23:12:23.693350   10064 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1022 23:12:23.693350   10064 start.go:495] detecting cgroup driver to use...
I1022 23:12:23.693350   10064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1022 23:12:23.696689   10064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:12:23.834979   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1022 23:12:23.909463   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1022 23:12:23.960435   10064 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1022 23:12:23.999807   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1022 23:12:24.073562   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1022 23:12:24.145282   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
W1022 23:12:24.188229   10064 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1022 23:12:24.191257   10064 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1022 23:12:24.243473   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1022 23:12:24.326266   10064 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1022 23:12:24.398328   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1022 23:12:24.471321   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1022 23:12:24.553016   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1022 23:12:24.623487   10064 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1022 23:12:24.687726   10064 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1022 23:12:24.755214   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:12:25.051322   10064 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1022 23:12:25.300048   10064 start.go:495] detecting cgroup driver to use...
I1022 23:12:25.300048   10064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1022 23:12:25.346424   10064 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1022 23:12:25.405527   10064 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1022 23:12:25.455434   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1022 23:12:25.543932   10064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:12:25.725423   10064 ssh_runner.go:195] Run: which cri-dockerd
I1022 23:12:25.815521   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1022 23:12:25.858352   10064 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1022 23:12:25.958906   10064 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1022 23:12:26.283858   10064 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1022 23:12:26.552466   10064 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1022 23:12:26.553000   10064 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1022 23:12:26.695179   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:12:27.069223   10064 ssh_runner.go:195] Run: sudo systemctl restart docker
I1022 23:12:29.625642   10064 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.5564195s)
I1022 23:12:29.661059   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1022 23:12:29.748810   10064 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1022 23:12:29.821737   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1022 23:12:29.891113   10064 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1022 23:12:30.245076   10064 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1022 23:12:30.498522   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:12:30.732595   10064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1022 23:12:30.812543   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1022 23:12:30.880017   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:12:31.146858   10064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1022 23:12:31.523851   10064 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1022 23:12:31.567500   10064 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1022 23:12:31.580629   10064 start.go:563] Will wait 60s for crictl version
I1022 23:12:31.618671   10064 ssh_runner.go:195] Run: which crictl
I1022 23:12:31.672341   10064 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1022 23:12:31.789809   10064 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1022 23:12:31.806112   10064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1022 23:12:31.900271   10064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1022 23:12:31.967388   10064 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1022 23:12:31.986657   10064 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1022 23:12:32.553131   10064 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1022 23:12:32.614311   10064 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1022 23:12:32.626522   10064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:12:32.705629   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:12:32.839725   10064 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Usuario:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1022 23:12:32.840284   10064 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1022 23:12:32.858452   10064 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1022 23:12:32.931260   10064 docker.go:685] Got preloaded images: -- stdout --
ghcr.io/trijuank/labpratorio2ds3:latest
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1022 23:12:32.931260   10064 docker.go:615] Images already preloaded, skipping extraction
I1022 23:12:32.949670   10064 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1022 23:12:33.020750   10064 docker.go:685] Got preloaded images: -- stdout --
ghcr.io/trijuank/labpratorio2ds3:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1022 23:12:33.022056   10064 cache_images.go:84] Images are preloaded, skipping loading
I1022 23:12:33.022056   10064 kubeadm.go:934] updating node { 192.168.58.2 8443 v1.31.0 docker true true} ...
I1022 23:12:33.024263   10064 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1022 23:12:33.046538   10064 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1022 23:12:33.718866   10064 cni.go:84] Creating CNI manager for ""
I1022 23:12:33.718866   10064 cni.go:136] multinode detected (3 nodes found), recommending kindnet
I1022 23:12:33.719431   10064 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1022 23:12:33.719431   10064 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1022 23:12:33.720036   10064 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1022 23:12:33.778068   10064 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1022 23:12:33.823066   10064 binaries.go:44] Found k8s binaries, skipping transfer
I1022 23:12:33.886746   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1022 23:12:33.912503   10064 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1022 23:12:33.963041   10064 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1022 23:12:34.025388   10064 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1022 23:12:34.133198   10064 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I1022 23:12:34.144208   10064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:12:34.233107   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:12:34.601498   10064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1022 23:12:34.671391   10064 certs.go:68] Setting up C:\Users\Usuario\.minikube\profiles\minikube for IP: 192.168.58.2
I1022 23:12:34.671391   10064 certs.go:194] generating shared ca certs ...
I1022 23:12:34.672005   10064 certs.go:226] acquiring lock for ca certs: {Name:mkc1d3168a00c9da72b970adbedab8654978744d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:12:34.700487   10064 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Usuario\.minikube\ca.key
I1022 23:12:34.764133   10064 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Usuario\.minikube\proxy-client-ca.key
I1022 23:12:34.765584   10064 certs.go:256] generating profile certs ...
I1022 23:12:34.768459   10064 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\Usuario\.minikube\profiles\minikube\client.key
I1022 23:12:34.831818   10064 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\Usuario\.minikube\profiles\minikube\apiserver.key.502bbb95
I1022 23:12:34.877099   10064 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\Usuario\.minikube\profiles\minikube\proxy-client.key
I1022 23:12:34.886209   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\ca-key.pem (1679 bytes)
I1022 23:12:34.888084   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\ca.pem (1082 bytes)
I1022 23:12:34.893512   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\cert.pem (1123 bytes)
I1022 23:12:34.896453   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\key.pem (1679 bytes)
I1022 23:12:34.918194   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1022 23:12:35.006055   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1022 23:12:35.101357   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1022 23:12:35.198939   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1022 23:12:35.331130   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1022 23:12:35.461590   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1022 23:12:35.534490   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1022 23:12:35.632520   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1022 23:12:35.719570   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1022 23:12:35.827509   10064 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1022 23:12:35.956910   10064 ssh_runner.go:195] Run: openssl version
I1022 23:12:36.036865   10064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1022 23:12:36.319910   10064 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1022 23:12:36.339167   10064 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 10 19:22 /usr/share/ca-certificates/minikubeCA.pem
I1022 23:12:36.385960   10064 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1022 23:12:36.476077   10064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1022 23:12:36.572488   10064 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1022 23:12:36.631425   10064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1022 23:12:36.717170   10064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1022 23:12:36.821327   10064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1022 23:12:36.977384   10064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1022 23:12:37.091599   10064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1022 23:12:37.223133   10064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1022 23:12:37.259421   10064 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Usuario:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1022 23:12:37.294546   10064 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1022 23:12:37.434559   10064 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1022 23:12:37.468396   10064 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1022 23:12:37.469395   10064 kubeadm.go:593] restartPrimaryControlPlane start ...
I1022 23:12:37.538029   10064 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1022 23:12:37.601927   10064 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1022 23:12:37.649380   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:12:37.817668   10064 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:55675"
I1022 23:12:37.817668   10064 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:55675, want: 127.0.0.1:51538
I1022 23:12:37.831360   10064 kubeconfig.go:62] C:\Users\Usuario\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I1022 23:12:37.835039   10064 lock.go:35] WriteFile acquiring C:\Users\Usuario\.kube\config: {Name:mk1bbfa23436177e61ded2dd315ee676cae85502 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:12:37.989670   10064 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1022 23:12:38.035386   10064 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1022 23:12:38.035386   10064 kubeadm.go:597] duration metric: took 565.9903ms to restartPrimaryControlPlane
I1022 23:12:38.035386   10064 kubeadm.go:394] duration metric: took 776.9631ms to StartCluster
I1022 23:12:38.035386   10064 settings.go:142] acquiring lock: {Name:mkd9d6fc1be357f34a04b58aee18051ce39eb9e8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:12:38.035713   10064 settings.go:150] Updating kubeconfig:  C:\Users\Usuario\.kube\config
I1022 23:12:38.049446   10064 lock.go:35] WriteFile acquiring C:\Users\Usuario\.kube\config: {Name:mk1bbfa23436177e61ded2dd315ee676cae85502 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:12:38.052559   10064 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1022 23:12:38.052559   10064 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1022 23:12:38.053746   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:12:38.054353   10064 out.go:177] 🔎  Verifying Kubernetes components...
I1022 23:12:38.056470   10064 out.go:177] 🌟  Enabled addons: 
I1022 23:12:38.058338   10064 addons.go:510] duration metric: took 6.3485ms for enable addons: enabled=[]
I1022 23:12:38.135659   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:12:38.581335   10064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1022 23:12:38.656267   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:12:38.809488   10064 api_server.go:52] waiting for apiserver process to appear ...
I1022 23:12:38.858447   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:39.376374   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:39.849317   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:40.362451   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:40.871438   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:41.374471   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:41.882421   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:42.374895   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:42.862539   10064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1022 23:12:42.906100   10064 api_server.go:72] duration metric: took 4.8535406s to wait for apiserver process to appear ...
I1022 23:12:42.906100   10064 api_server.go:88] waiting for apiserver healthz status ...
I1022 23:12:42.908956   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:42.915490   10064 api_server.go:269] stopped: https://127.0.0.1:51538/healthz: Get "https://127.0.0.1:51538/healthz": EOF
I1022 23:12:43.411170   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:43.414440   10064 api_server.go:269] stopped: https://127.0.0.1:51538/healthz: Get "https://127.0.0.1:51538/healthz": EOF
I1022 23:12:43.918546   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:43.921841   10064 api_server.go:269] stopped: https://127.0.0.1:51538/healthz: Get "https://127.0.0.1:51538/healthz": EOF
I1022 23:12:44.418637   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:44.422270   10064 api_server.go:269] stopped: https://127.0.0.1:51538/healthz: Get "https://127.0.0.1:51538/healthz": EOF
I1022 23:12:44.913475   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:49.619027   10064 api_server.go:279] https://127.0.0.1:51538/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1022 23:12:49.619547   10064 api_server.go:103] status: https://127.0.0.1:51538/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1022 23:12:49.619568   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:49.664651   10064 api_server.go:279] https://127.0.0.1:51538/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1022 23:12:49.664651   10064 api_server.go:103] status: https://127.0.0.1:51538/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1022 23:12:49.915877   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:49.930323   10064 api_server.go:279] https://127.0.0.1:51538/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1022 23:12:49.930323   10064 api_server.go:103] status: https://127.0.0.1:51538/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1022 23:12:50.423148   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:50.452214   10064 api_server.go:279] https://127.0.0.1:51538/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1022 23:12:50.452214   10064 api_server.go:103] status: https://127.0.0.1:51538/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1022 23:12:50.917945   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:50.935027   10064 api_server.go:279] https://127.0.0.1:51538/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1022 23:12:50.935027   10064 api_server.go:103] status: https://127.0.0.1:51538/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1022 23:12:51.416799   10064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51538/healthz ...
I1022 23:12:51.431050   10064 api_server.go:279] https://127.0.0.1:51538/healthz returned 200:
ok
I1022 23:12:51.468209   10064 api_server.go:141] control plane version: v1.31.0
I1022 23:12:51.468209   10064 api_server.go:131] duration metric: took 8.5621092s to wait for apiserver health ...
I1022 23:12:51.468785   10064 system_pods.go:43] waiting for kube-system pods to appear ...
I1022 23:12:51.508977   10064 system_pods.go:59] 9 kube-system pods found
I1022 23:12:51.508977   10064 system_pods.go:61] "coredns-6f6b679f8f-ps28w" [f3c820ca-cb96-4b42-bb9c-b2a712c30b97] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "etcd-minikube" [ad86af50-ae9d-4d34-bc13-3360ddf7d027] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "kube-apiserver-minikube" [f582f346-7a5b-4bc9-902c-53872cf80f72] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "kube-controller-manager-minikube" [21d104ff-146d-4a6c-b50a-6ef0a138cea1] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "kube-proxy-l9ksf" [6860b091-21db-4dc6-ad42-8a33af05ce35] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "kube-proxy-qgd82" [961ac5f9-0db8-4387-8d9c-31d67a14cf58] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "kube-proxy-v7jrx" [d80b54f1-16ed-410e-9df2-9fa63c0dd7ea] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "kube-scheduler-minikube" [eaa0e0c2-6a10-4318-803d-3ca3ada01bc4] Running
I1022 23:12:51.508977   10064 system_pods.go:61] "storage-provisioner" [6afedde4-a5a7-4ba1-a9a1-825113089584] Running
I1022 23:12:51.508977   10064 system_pods.go:74] duration metric: took 40.192ms to wait for pod list to return data ...
I1022 23:12:51.508977   10064 kubeadm.go:582] duration metric: took 13.4564172s to wait for: map[apiserver:true system_pods:true]
I1022 23:12:51.508977   10064 node_conditions.go:102] verifying NodePressure condition ...
I1022 23:12:51.523783   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:12:51.523783   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:12:51.523783   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:12:51.523783   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:12:51.523783   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:12:51.523783   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:12:51.523783   10064 node_conditions.go:105] duration metric: took 14.8064ms to run NodePressure ...
I1022 23:12:51.523783   10064 start.go:241] waiting for startup goroutines ...
I1022 23:12:51.523783   10064 start.go:246] waiting for cluster config update ...
I1022 23:12:51.523783   10064 start.go:255] writing updated cluster config ...
I1022 23:12:51.526135   10064 out.go:201] 
I1022 23:12:51.557476   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:12:51.586874   10064 config.go:182] Loaded profile config "nodes": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:12:51.587050   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
I1022 23:12:51.596634   10064 out.go:177] 👍  Starting "minikube-m02" worker node in "minikube" cluster
I1022 23:12:51.598828   10064 cache.go:121] Beginning downloading kic base image for docker with docker
I1022 23:12:51.599729   10064 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1022 23:12:51.602062   10064 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1022 23:12:51.602062   10064 cache.go:56] Caching tarball of preloaded images
I1022 23:12:51.602062   10064 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1022 23:12:51.602603   10064 preload.go:172] Found C:\Users\Usuario\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1022 23:12:51.602740   10064 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1022 23:12:51.603266   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
W1022 23:12:51.954716   10064 image.go:95] image docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1022 23:12:51.954716   10064 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1022 23:12:51.955277   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:12:51.955836   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:12:51.955836   10064 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1022 23:12:51.955836   10064 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1022 23:12:51.955836   10064 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1022 23:12:51.955836   10064 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1022 23:12:51.955836   10064 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1022 23:12:51.956414   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:12:55.794323   10064 cache.go:164] successfully loaded and using docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1022 23:12:55.794323   10064 cache.go:194] Successfully downloaded all kic artifacts
I1022 23:12:55.794949   10064 start.go:360] acquireMachinesLock for minikube-m02: {Name:mk916e9977f2965466ba98bda4629082b3727e42 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1022 23:12:55.795624   10064 start.go:364] duration metric: took 675.5µs to acquireMachinesLock for "minikube-m02"
I1022 23:12:55.796202   10064 start.go:96] Skipping create...Using existing machine configuration
I1022 23:12:55.796202   10064 fix.go:54] fixHost starting: m02
I1022 23:12:55.870506   10064 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1022 23:12:56.046300   10064 fix.go:112] recreateIfNeeded on minikube-m02: state=Stopped err=<nil>
W1022 23:12:56.046300   10064 fix.go:138] unexpected machine state, will restart: <nil>
I1022 23:12:56.047430   10064 out.go:177] 🔄  Restarting existing docker container for "minikube-m02" ...
I1022 23:12:56.072332   10064 cli_runner.go:164] Run: docker start minikube-m02
I1022 23:13:02.175598   10064 cli_runner.go:217] Completed: docker start minikube-m02: (6.1032663s)
I1022 23:13:02.213392   10064 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1022 23:13:02.532179   10064 kic.go:430] container "minikube-m02" state is running.
I1022 23:13:02.574067   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1022 23:13:02.792821   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
I1022 23:13:02.805280   10064 machine.go:93] provisionDockerMachine start ...
I1022 23:13:02.837432   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:03.074406   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:03.098155   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51582 <nil> <nil>}
I1022 23:13:03.098410   10064 main.go:141] libmachine: About to run SSH command:
hostname
I1022 23:13:03.114239   10064 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1022 23:13:06.128715   10064 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1022 23:13:09.503519   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I1022 23:13:09.503519   10064 ubuntu.go:169] provisioning hostname "minikube-m02"
I1022 23:13:09.529162   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:09.670433   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:09.670962   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51582 <nil> <nil>}
I1022 23:13:09.670962   10064 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I1022 23:13:10.217957   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I1022 23:13:10.242381   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:10.390808   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:10.391365   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51582 <nil> <nil>}
I1022 23:13:10.391365   10064 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I1022 23:13:10.938137   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1022 23:13:10.938137   10064 ubuntu.go:175] set auth options {CertDir:C:\Users\Usuario\.minikube CaCertPath:C:\Users\Usuario\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Usuario\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Usuario\.minikube\machines\server.pem ServerKeyPath:C:\Users\Usuario\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Usuario\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Usuario\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Usuario\.minikube}
I1022 23:13:10.938137   10064 ubuntu.go:177] setting up certificates
I1022 23:13:10.938137   10064 provision.go:84] configureAuth start
I1022 23:13:10.960916   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1022 23:13:11.088920   10064 provision.go:143] copyHostCerts
I1022 23:13:11.089453   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/ca.pem, removing ...
I1022 23:13:11.089453   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\ca.pem
I1022 23:13:11.090637   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\ca.pem --> C:\Users\Usuario\.minikube/ca.pem (1082 bytes)
I1022 23:13:11.093460   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/cert.pem, removing ...
I1022 23:13:11.093460   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\cert.pem
I1022 23:13:11.093994   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\cert.pem --> C:\Users\Usuario\.minikube/cert.pem (1123 bytes)
I1022 23:13:11.097238   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/key.pem, removing ...
I1022 23:13:11.097238   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\key.pem
I1022 23:13:11.097238   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\key.pem --> C:\Users\Usuario\.minikube/key.pem (1679 bytes)
I1022 23:13:11.099940   10064 provision.go:117] generating server cert: C:\Users\Usuario\.minikube\machines\server.pem ca-key=C:\Users\Usuario\.minikube\certs\ca.pem private-key=C:\Users\Usuario\.minikube\certs\ca-key.pem org=Usuario.minikube-m02 san=[127.0.0.1 192.168.58.3 localhost minikube minikube-m02]
I1022 23:13:11.469010   10064 provision.go:177] copyRemoteCerts
I1022 23:13:11.539592   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1022 23:13:11.560324   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:11.688953   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51582 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1022 23:13:11.929797   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1082 bytes)
I1022 23:13:12.026452   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\machines\server.pem --> /etc/docker/server.pem (1208 bytes)
I1022 23:13:12.147339   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1022 23:13:12.258958   10064 provision.go:87] duration metric: took 1.3208207s to configureAuth
I1022 23:13:12.258958   10064 ubuntu.go:193] setting minikube options for container-runtime
I1022 23:13:12.261309   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:12.285714   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:12.450473   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:12.451593   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51582 <nil> <nil>}
I1022 23:13:12.451642   10064 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1022 23:13:12.725940   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1022 23:13:12.725940   10064 ubuntu.go:71] root file system type: overlay
I1022 23:13:12.726622   10064 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1022 23:13:12.751578   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:13.042261   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:13.042261   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51582 <nil> <nil>}
I1022 23:13:13.042261   10064 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.58.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1022 23:13:13.516769   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.58.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1022 23:13:13.546238   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:13.855301   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:13.856027   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51582 <nil> <nil>}
I1022 23:13:13.856027   10064 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1022 23:13:14.207609   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1022 23:13:14.207609   10064 machine.go:96] duration metric: took 11.4023288s to provisionDockerMachine
I1022 23:13:14.208124   10064 start.go:293] postStartSetup for "minikube-m02" (driver="docker")
I1022 23:13:14.208124   10064 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1022 23:13:14.255235   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1022 23:13:14.282539   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:14.403361   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51582 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1022 23:13:14.622420   10064 ssh_runner.go:195] Run: cat /etc/os-release
I1022 23:13:14.636240   10064 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1022 23:13:14.636240   10064 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1022 23:13:14.636240   10064 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1022 23:13:14.636240   10064 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1022 23:13:14.636772   10064 filesync.go:126] Scanning C:\Users\Usuario\.minikube\addons for local assets ...
I1022 23:13:14.637325   10064 filesync.go:126] Scanning C:\Users\Usuario\.minikube\files for local assets ...
I1022 23:13:14.637843   10064 start.go:296] duration metric: took 429.2017ms for postStartSetup
I1022 23:13:14.678069   10064 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1022 23:13:14.698620   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:14.815458   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51582 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1022 23:13:15.010366   10064 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1022 23:13:15.032472   10064 fix.go:56] duration metric: took 19.2362699s for fixHost
I1022 23:13:15.032982   10064 start.go:83] releasing machines lock for "minikube-m02", held for 19.2368476s
I1022 23:13:15.050623   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1022 23:13:15.222351   10064 out.go:177] 🌐  Found network options:
I1022 23:13:15.224590   10064 out.go:177]     ▪ NO_PROXY=192.168.58.2
W1022 23:13:15.229099   10064 proxy.go:119] fail to check proxy env: Error ip not in block
I1022 23:13:15.231895   10064 out.go:177]     ▪ NO_PROXY=192.168.58.2
W1022 23:13:15.233825   10064 proxy.go:119] fail to check proxy env: Error ip not in block
W1022 23:13:15.234449   10064 proxy.go:119] fail to check proxy env: Error ip not in block
I1022 23:13:15.250600   10064 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1022 23:13:15.300579   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:15.319767   10064 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1022 23:13:15.359177   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1022 23:13:15.525757   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51582 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1022 23:13:15.592204   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51582 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m02\id_rsa Username:docker}
W1022 23:13:15.720775   10064 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1022 23:13:15.834099   10064 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1022 23:13:15.861901   10064 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1022 23:13:15.900242   10064 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1022 23:13:15.937189   10064 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1022 23:13:15.937189   10064 start.go:495] detecting cgroup driver to use...
I1022 23:13:15.937189   10064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1022 23:13:15.937399   10064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:13:16.031443   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1022 23:13:16.107458   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1022 23:13:16.149851   10064 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1022 23:13:16.193780   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1022 23:13:16.281501   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W1022 23:13:16.328420   10064 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1022 23:13:16.329358   10064 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1022 23:13:16.391293   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1022 23:13:16.463665   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1022 23:13:16.546486   10064 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1022 23:13:16.630633   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1022 23:13:16.733633   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1022 23:13:16.809135   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1022 23:13:16.886816   10064 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1022 23:13:16.975133   10064 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1022 23:13:17.063104   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:17.394084   10064 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1022 23:13:17.896825   10064 start.go:495] detecting cgroup driver to use...
I1022 23:13:17.896825   10064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1022 23:13:17.952080   10064 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1022 23:13:18.043972   10064 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1022 23:13:18.100905   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1022 23:13:18.163734   10064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:13:18.296021   10064 ssh_runner.go:195] Run: which cri-dockerd
I1022 23:13:18.364065   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1022 23:13:18.399069   10064 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1022 23:13:18.562597   10064 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1022 23:13:19.007164   10064 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1022 23:13:19.481029   10064 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1022 23:13:19.481029   10064 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1022 23:13:19.690472   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:20.209045   10064 ssh_runner.go:195] Run: sudo systemctl restart docker
I1022 23:13:22.518809   10064 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.3097639s)
I1022 23:13:22.570586   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1022 23:13:22.710033   10064 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1022 23:13:22.829366   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1022 23:13:22.927402   10064 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1022 23:13:23.281321   10064 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1022 23:13:23.670918   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:24.050186   10064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1022 23:13:24.171857   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1022 23:13:24.317063   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:24.760830   10064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1022 23:13:25.184813   10064 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1022 23:13:25.236324   10064 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1022 23:13:25.252568   10064 start.go:563] Will wait 60s for crictl version
I1022 23:13:25.301762   10064 ssh_runner.go:195] Run: which crictl
I1022 23:13:25.365656   10064 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1022 23:13:25.914279   10064 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1022 23:13:25.942085   10064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1022 23:13:26.548102   10064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1022 23:13:26.722409   10064 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1022 23:13:26.728121   10064 out.go:177]     ▪ env NO_PROXY=192.168.58.2
I1022 23:13:26.791867   10064 cli_runner.go:164] Run: docker exec -t minikube-m02 dig +short host.docker.internal
I1022 23:13:27.327285   10064 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1022 23:13:27.379283   10064 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1022 23:13:27.400012   10064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:13:27.452040   10064 mustload.go:65] Loading cluster: minikube
I1022 23:13:27.453951   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:27.504707   10064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:13:27.666616   10064 host.go:66] Checking if "minikube" exists ...
I1022 23:13:27.689818   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:13:27.861217   10064 certs.go:68] Setting up C:\Users\Usuario\.minikube\profiles\minikube for IP: 192.168.58.3
I1022 23:13:27.861217   10064 certs.go:194] generating shared ca certs ...
I1022 23:13:27.861832   10064 certs.go:226] acquiring lock for ca certs: {Name:mkc1d3168a00c9da72b970adbedab8654978744d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:13:27.871499   10064 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Usuario\.minikube\ca.key
I1022 23:13:27.876058   10064 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Usuario\.minikube\proxy-client-ca.key
I1022 23:13:27.878427   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\ca-key.pem (1679 bytes)
I1022 23:13:27.878427   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\ca.pem (1082 bytes)
I1022 23:13:27.879294   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\cert.pem (1123 bytes)
I1022 23:13:27.879294   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\key.pem (1679 bytes)
I1022 23:13:27.880351   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1022 23:13:27.980679   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1022 23:13:28.089686   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1022 23:13:28.207165   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1022 23:13:28.335528   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1022 23:13:28.518135   10064 ssh_runner.go:195] Run: openssl version
I1022 23:13:28.608788   10064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1022 23:13:28.749067   10064 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1022 23:13:28.779628   10064 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 10 19:22 /usr/share/ca-certificates/minikubeCA.pem
I1022 23:13:28.856310   10064 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1022 23:13:28.967094   10064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1022 23:13:29.054976   10064 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1022 23:13:29.072456   10064 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1022 23:13:29.072510   10064 kubeadm.go:934] updating node {m02 192.168.58.3 0 v1.31.0  false true} ...
I1022 23:13:29.073076   10064 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.3

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1022 23:13:29.121053   10064 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1022 23:13:29.160538   10064 binaries.go:44] Found k8s binaries, skipping transfer
I1022 23:13:29.217182   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1022 23:13:29.254996   10064 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (311 bytes)
I1022 23:13:29.338617   10064 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1022 23:13:29.466114   10064 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I1022 23:13:29.486966   10064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:13:29.597281   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:30.079169   10064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1022 23:13:30.128340   10064 start.go:235] Will wait 6m0s for node &{Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime: ControlPlane:false Worker:true}
I1022 23:13:30.129717   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:30.130135   10064 out.go:177] 🔎  Verifying Kubernetes components...
I1022 23:13:30.189681   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:30.535469   10064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1022 23:13:30.617720   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:13:30.766659   10064 kubeadm.go:582] duration metric: took 638.3187ms to wait for: map[apiserver:true system_pods:true]
I1022 23:13:30.766659   10064 node_conditions.go:102] verifying NodePressure condition ...
I1022 23:13:30.788167   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:13:30.788167   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:13:30.788167   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:13:30.788167   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:13:30.788167   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:13:30.788167   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:13:30.788167   10064 node_conditions.go:105] duration metric: took 21.5083ms to run NodePressure ...
I1022 23:13:30.788167   10064 start.go:241] waiting for startup goroutines ...
I1022 23:13:30.788733   10064 start.go:255] writing updated cluster config ...
I1022 23:13:30.793051   10064 out.go:201] 
I1022 23:13:30.856572   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:30.859405   10064 config.go:182] Loaded profile config "nodes": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:30.860457   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
I1022 23:13:30.885812   10064 out.go:177] 👍  Starting "minikube-m03" worker node in "minikube" cluster
I1022 23:13:30.895826   10064 cache.go:121] Beginning downloading kic base image for docker with docker
I1022 23:13:30.897078   10064 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1022 23:13:30.924533   10064 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1022 23:13:30.924533   10064 cache.go:56] Caching tarball of preloaded images
I1022 23:13:30.924652   10064 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1022 23:13:30.925182   10064 preload.go:172] Found C:\Users\Usuario\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1022 23:13:30.940806   10064 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1022 23:13:30.940806   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
W1022 23:13:31.266671   10064 image.go:95] image docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1022 23:13:31.266671   10064 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1022 23:13:31.266671   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:13:31.267287   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:13:31.267287   10064 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1022 23:13:31.267287   10064 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1022 23:13:31.267287   10064 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1022 23:13:31.267826   10064 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1022 23:13:31.267826   10064 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1022 23:13:31.267826   10064 localpath.go:151] windows sanitize: C:\Users\Usuario\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Usuario\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1022 23:13:34.789250   10064 cache.go:164] successfully loaded and using docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1022 23:13:34.789317   10064 cache.go:194] Successfully downloaded all kic artifacts
I1022 23:13:34.790150   10064 start.go:360] acquireMachinesLock for minikube-m03: {Name:mkc45a34c5579b82bf0c9a1c72946078bf825368 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1022 23:13:34.790190   10064 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube-m03"
I1022 23:13:34.790771   10064 start.go:96] Skipping create...Using existing machine configuration
I1022 23:13:34.790771   10064 fix.go:54] fixHost starting: m03
I1022 23:13:34.858205   10064 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1022 23:13:35.140773   10064 fix.go:112] recreateIfNeeded on minikube-m03: state=Stopped err=<nil>
W1022 23:13:35.140773   10064 fix.go:138] unexpected machine state, will restart: <nil>
I1022 23:13:35.142542   10064 out.go:177] 🔄  Restarting existing docker container for "minikube-m03" ...
I1022 23:13:35.174888   10064 cli_runner.go:164] Run: docker start minikube-m03
I1022 23:13:38.069022   10064 cli_runner.go:217] Completed: docker start minikube-m03: (2.8941335s)
I1022 23:13:38.090850   10064 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1022 23:13:38.338589   10064 kic.go:430] container "minikube-m03" state is running.
I1022 23:13:38.366961   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1022 23:13:38.515433   10064 profile.go:143] Saving config to C:\Users\Usuario\.minikube\profiles\minikube\config.json ...
I1022 23:13:38.520622   10064 machine.go:93] provisionDockerMachine start ...
I1022 23:13:38.547352   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:38.720444   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:38.746773   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51625 <nil> <nil>}
I1022 23:13:38.746773   10064 main.go:141] libmachine: About to run SSH command:
hostname
I1022 23:13:38.749259   10064 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1022 23:13:42.035603   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I1022 23:13:42.035603   10064 ubuntu.go:169] provisioning hostname "minikube-m03"
I1022 23:13:42.056746   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:42.239986   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:42.240568   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51625 <nil> <nil>}
I1022 23:13:42.240568   10064 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I1022 23:13:42.608121   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I1022 23:13:42.630614   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:42.799645   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:42.800367   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51625 <nil> <nil>}
I1022 23:13:42.800367   10064 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I1022 23:13:43.004039   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1022 23:13:43.004039   10064 ubuntu.go:175] set auth options {CertDir:C:\Users\Usuario\.minikube CaCertPath:C:\Users\Usuario\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Usuario\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Usuario\.minikube\machines\server.pem ServerKeyPath:C:\Users\Usuario\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Usuario\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Usuario\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Usuario\.minikube}
I1022 23:13:43.004039   10064 ubuntu.go:177] setting up certificates
I1022 23:13:43.004555   10064 provision.go:84] configureAuth start
I1022 23:13:43.031213   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1022 23:13:43.177414   10064 provision.go:143] copyHostCerts
I1022 23:13:43.178501   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/ca.pem, removing ...
I1022 23:13:43.179023   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\ca.pem
I1022 23:13:43.179651   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\ca.pem --> C:\Users\Usuario\.minikube/ca.pem (1082 bytes)
I1022 23:13:43.181523   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/cert.pem, removing ...
I1022 23:13:43.181523   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\cert.pem
I1022 23:13:43.181523   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\cert.pem --> C:\Users\Usuario\.minikube/cert.pem (1123 bytes)
I1022 23:13:43.184253   10064 exec_runner.go:144] found C:\Users\Usuario\.minikube/key.pem, removing ...
I1022 23:13:43.184253   10064 exec_runner.go:203] rm: C:\Users\Usuario\.minikube\key.pem
I1022 23:13:43.185561   10064 exec_runner.go:151] cp: C:\Users\Usuario\.minikube\certs\key.pem --> C:\Users\Usuario\.minikube/key.pem (1679 bytes)
I1022 23:13:43.187944   10064 provision.go:117] generating server cert: C:\Users\Usuario\.minikube\machines\server.pem ca-key=C:\Users\Usuario\.minikube\certs\ca.pem private-key=C:\Users\Usuario\.minikube\certs\ca-key.pem org=Usuario.minikube-m03 san=[127.0.0.1 192.168.58.4 localhost minikube minikube-m03]
I1022 23:13:43.432648   10064 provision.go:177] copyRemoteCerts
I1022 23:13:43.487979   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1022 23:13:43.511087   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:43.646441   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51625 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m03\id_rsa Username:docker}
I1022 23:13:43.831007   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1082 bytes)
I1022 23:13:43.933936   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\machines\server.pem --> /etc/docker/server.pem (1204 bytes)
I1022 23:13:44.027959   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1022 23:13:44.121381   10064 provision.go:87] duration metric: took 1.116807s to configureAuth
I1022 23:13:44.121381   10064 ubuntu.go:193] setting minikube options for container-runtime
I1022 23:13:44.122283   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:44.143445   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:44.287332   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:44.288404   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51625 <nil> <nil>}
I1022 23:13:44.288404   10064 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1022 23:13:44.576333   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1022 23:13:44.576333   10064 ubuntu.go:71] root file system type: overlay
I1022 23:13:44.576333   10064 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1022 23:13:44.599719   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:44.761628   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:44.762731   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51625 <nil> <nil>}
I1022 23:13:44.762731   10064 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.58.2"
Environment="NO_PROXY=192.168.58.2,192.168.58.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1022 23:13:45.046002   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.58.2
Environment=NO_PROXY=192.168.58.2,192.168.58.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1022 23:13:45.067526   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:45.228687   10064 main.go:141] libmachine: Using SSH client type: native
I1022 23:13:45.229291   10064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xabc9c0] 0xabf5a0 <nil>  [] 0s} 127.0.0.1 51625 <nil> <nil>}
I1022 23:13:45.229425   10064 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1022 23:13:45.504093   10064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1022 23:13:45.504199   10064 machine.go:96] duration metric: took 6.9835772s to provisionDockerMachine
I1022 23:13:45.504199   10064 start.go:293] postStartSetup for "minikube-m03" (driver="docker")
I1022 23:13:45.504199   10064 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1022 23:13:45.551169   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1022 23:13:45.573196   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:45.727065   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51625 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m03\id_rsa Username:docker}
I1022 23:13:45.963179   10064 ssh_runner.go:195] Run: cat /etc/os-release
I1022 23:13:45.979974   10064 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1022 23:13:45.979974   10064 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1022 23:13:45.979974   10064 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1022 23:13:45.979974   10064 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1022 23:13:45.980500   10064 filesync.go:126] Scanning C:\Users\Usuario\.minikube\addons for local assets ...
I1022 23:13:45.980635   10064 filesync.go:126] Scanning C:\Users\Usuario\.minikube\files for local assets ...
I1022 23:13:45.981194   10064 start.go:296] duration metric: took 476.9947ms for postStartSetup
I1022 23:13:46.027783   10064 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1022 23:13:46.049856   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:46.186329   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51625 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m03\id_rsa Username:docker}
I1022 23:13:46.403167   10064 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1022 23:13:46.418120   10064 fix.go:56] duration metric: took 11.6273494s for fixHost
I1022 23:13:46.418120   10064 start.go:83] releasing machines lock for "minikube-m03", held for 11.62793s
I1022 23:13:46.442399   10064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1022 23:13:46.566422   10064 out.go:177] 🌐  Found network options:
I1022 23:13:46.568766   10064 out.go:177]     ▪ NO_PROXY=192.168.58.2,192.168.58.3
W1022 23:13:46.571366   10064 proxy.go:119] fail to check proxy env: Error ip not in block
W1022 23:13:46.571366   10064 proxy.go:119] fail to check proxy env: Error ip not in block
I1022 23:13:46.572019   10064 out.go:177]     ▪ NO_PROXY=192.168.58.2,192.168.58.3
W1022 23:13:46.575583   10064 proxy.go:119] fail to check proxy env: Error ip not in block
W1022 23:13:46.575583   10064 proxy.go:119] fail to check proxy env: Error ip not in block
W1022 23:13:46.575583   10064 proxy.go:119] fail to check proxy env: Error ip not in block
W1022 23:13:46.575583   10064 proxy.go:119] fail to check proxy env: Error ip not in block
I1022 23:13:46.618480   10064 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1022 23:13:46.670153   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:46.723916   10064 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1022 23:13:46.749054   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1022 23:13:46.936211   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51625 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m03\id_rsa Username:docker}
I1022 23:13:46.944820   10064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51625 SSHKeyPath:C:\Users\Usuario\.minikube\machines\minikube-m03\id_rsa Username:docker}
W1022 23:13:47.148453   10064 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1022 23:13:47.201181   10064 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1022 23:13:47.235900   10064 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1022 23:13:47.282263   10064 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1022 23:13:47.319329   10064 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1022 23:13:47.319329   10064 start.go:495] detecting cgroup driver to use...
I1022 23:13:47.319329   10064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1022 23:13:47.319877   10064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:13:47.425751   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1022 23:13:47.496253   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1022 23:13:47.541424   10064 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1022 23:13:47.590164   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1022 23:13:47.674408   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W1022 23:13:47.729445   10064 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1022 23:13:47.730676   10064 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1022 23:13:47.768407   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1022 23:13:47.846338   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1022 23:13:47.930958   10064 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1022 23:13:48.056575   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1022 23:13:48.133080   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1022 23:13:48.203414   10064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1022 23:13:48.280371   10064 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1022 23:13:48.364605   10064 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1022 23:13:48.446688   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:48.761848   10064 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1022 23:13:49.157454   10064 start.go:495] detecting cgroup driver to use...
I1022 23:13:49.157454   10064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1022 23:13:49.203418   10064 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1022 23:13:49.246220   10064 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1022 23:13:49.292566   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1022 23:13:49.339446   10064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:13:49.432542   10064 ssh_runner.go:195] Run: which cri-dockerd
I1022 23:13:49.495766   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1022 23:13:49.523122   10064 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1022 23:13:49.674913   10064 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1022 23:13:50.104962   10064 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1022 23:13:50.663392   10064 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1022 23:13:50.663392   10064 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1022 23:13:50.854194   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:51.503951   10064 ssh_runner.go:195] Run: sudo systemctl restart docker
I1022 23:13:54.186982   10064 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.6830312s)
I1022 23:13:54.241825   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1022 23:13:54.368474   10064 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1022 23:13:54.494982   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1022 23:13:54.587425   10064 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1022 23:13:55.012002   10064 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1022 23:13:55.328406   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:55.769727   10064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1022 23:13:55.902748   10064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1022 23:13:55.985968   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:56.288159   10064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1022 23:13:56.488958   10064 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1022 23:13:56.537812   10064 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1022 23:13:56.556089   10064 start.go:563] Will wait 60s for crictl version
I1022 23:13:56.601005   10064 ssh_runner.go:195] Run: which crictl
I1022 23:13:56.672050   10064 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1022 23:13:56.809339   10064 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1022 23:13:56.834530   10064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1022 23:13:56.950784   10064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1022 23:13:57.059829   10064 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1022 23:13:57.062049   10064 out.go:177]     ▪ env NO_PROXY=192.168.58.2
I1022 23:13:57.064953   10064 out.go:177]     ▪ env NO_PROXY=192.168.58.2,192.168.58.3
I1022 23:13:57.102022   10064 cli_runner.go:164] Run: docker exec -t minikube-m03 dig +short host.docker.internal
I1022 23:13:57.478715   10064 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1022 23:13:57.524362   10064 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1022 23:13:57.538862   10064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:13:57.573558   10064 mustload.go:65] Loading cluster: minikube
I1022 23:13:57.575233   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:57.618258   10064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:13:57.745049   10064 host.go:66] Checking if "minikube" exists ...
I1022 23:13:57.766742   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:13:57.955998   10064 certs.go:68] Setting up C:\Users\Usuario\.minikube\profiles\minikube for IP: 192.168.58.4
I1022 23:13:57.955998   10064 certs.go:194] generating shared ca certs ...
I1022 23:13:57.955998   10064 certs.go:226] acquiring lock for ca certs: {Name:mkc1d3168a00c9da72b970adbedab8654978744d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:13:57.958769   10064 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Usuario\.minikube\ca.key
I1022 23:13:57.960394   10064 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Usuario\.minikube\proxy-client-ca.key
I1022 23:13:57.961573   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\ca-key.pem (1679 bytes)
I1022 23:13:57.962684   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\ca.pem (1082 bytes)
I1022 23:13:57.962684   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\cert.pem (1123 bytes)
I1022 23:13:57.964986   10064 certs.go:484] found cert: C:\Users\Usuario\.minikube\certs\key.pem (1679 bytes)
I1022 23:13:57.967070   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1022 23:13:58.135735   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1022 23:13:58.219512   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1022 23:13:58.306844   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1022 23:13:58.409736   10064 ssh_runner.go:362] scp C:\Users\Usuario\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1022 23:13:58.582217   10064 ssh_runner.go:195] Run: openssl version
I1022 23:13:58.648590   10064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1022 23:13:58.725585   10064 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1022 23:13:58.739222   10064 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 10 19:22 /usr/share/ca-certificates/minikubeCA.pem
I1022 23:13:58.787342   10064 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1022 23:13:58.855397   10064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1022 23:13:58.939048   10064 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1022 23:13:58.954273   10064 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1022 23:13:58.954822   10064 kubeadm.go:934] updating node {m03 192.168.58.4 0 v1.31.0  false true} ...
I1022 23:13:58.954822   10064 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.4

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1022 23:13:58.999437   10064 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1022 23:13:59.033364   10064 binaries.go:44] Found k8s binaries, skipping transfer
I1022 23:13:59.076033   10064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1022 23:13:59.113156   10064 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (311 bytes)
I1022 23:13:59.165567   10064 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1022 23:13:59.279569   10064 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I1022 23:13:59.291335   10064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:13:59.367723   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:13:59.731291   10064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1022 23:13:59.895135   10064 start.go:235] Will wait 6m0s for node &{Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.31.0 ContainerRuntime: ControlPlane:false Worker:true}
I1022 23:13:59.919159   10064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1022 23:13:59.927619   10064 out.go:177] 🔎  Verifying Kubernetes components...
I1022 23:14:00.017421   10064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1022 23:14:00.664524   10064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1022 23:14:00.888653   10064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:14:01.144858   10064 kubeadm.go:582] duration metric: took 1.2495897s to wait for: map[apiserver:true system_pods:true]
I1022 23:14:01.144858   10064 node_conditions.go:102] verifying NodePressure condition ...
I1022 23:14:01.290679   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:14:01.290679   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:14:01.290679   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:14:01.290679   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:14:01.290679   10064 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1022 23:14:01.291679   10064 node_conditions.go:123] node cpu capacity is 4
I1022 23:14:01.291679   10064 node_conditions.go:105] duration metric: took 146.8211ms to run NodePressure ...
I1022 23:14:01.291679   10064 start.go:241] waiting for startup goroutines ...
I1022 23:14:01.291679   10064 start.go:255] writing updated cluster config ...
I1022 23:14:01.422182   10064 ssh_runner.go:195] Run: rm -f paused
I1022 23:14:01.895360   10064 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I1022 23:14:01.896760   10064 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 23 04:12:31 minikube cri-dockerd[1394]: time="2024-10-23T04:12:31Z" level=info msg="Start cri-dockerd grpc backend"
Oct 23 04:12:31 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 23 04:12:37 minikube cri-dockerd[1394]: time="2024-10-23T04:12:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-tsv5l_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9cba9665ccdd3225eeaa4d4cc7086da23b01c0328b510c6213862d47410914a2\""
Oct 23 04:12:37 minikube cri-dockerd[1394]: time="2024-10-23T04:12:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-tsv5l_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a8952fbfe92066c0ff04cafa31cbdf22f7826185eeef6cf0200ac0ef19b5e74f\""
Oct 23 04:12:37 minikube cri-dockerd[1394]: time="2024-10-23T04:12:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"service-c-6bc8b97877-z5jp9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9adcad27284f776395e2c4fa0d3aeb378bfefa7bf33523bde5d0dafa545f0758\""
Oct 23 04:12:37 minikube cri-dockerd[1394]: time="2024-10-23T04:12:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-ps28w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"00a596396f8309bf1f5efe66704be12dd27b74cd49991c12c299c1b6aefbc070\""
Oct 23 04:12:38 minikube cri-dockerd[1394]: time="2024-10-23T04:12:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-ps28w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"db30a1cb7ee4b1b6c18668c1eab856b5c42df98f88a61b87f5cadbd4faa04abd\""
Oct 23 04:12:38 minikube cri-dockerd[1394]: time="2024-10-23T04:12:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-695b96c756-4lnrb_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4598c152cb846e1308b2b66424c7f504c20ccbafcdaf5e07b9256adce3f5c235\""
Oct 23 04:12:41 minikube cri-dockerd[1394]: time="2024-10-23T04:12:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1191245b20c83ef4d5f944afc83634d887278f3576730354cf92849d8c9409f7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:12:41 minikube cri-dockerd[1394]: time="2024-10-23T04:12:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/00a64e8b4dd6d87bccd0f62de015929ace7a8f084b90c3252fb2397960b2ca4b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:12:41 minikube cri-dockerd[1394]: time="2024-10-23T04:12:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47e13b0244b4b28b4c142c96cc8b8045e360abebf49984fafa73cb8582e4714c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:12:41 minikube cri-dockerd[1394]: time="2024-10-23T04:12:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c4dd0c8b77b9d1728a749f552ed3ad51936196026aed25d9df33f02e339249e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:12:41 minikube cri-dockerd[1394]: time="2024-10-23T04:12:41Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-ps28w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"00a596396f8309bf1f5efe66704be12dd27b74cd49991c12c299c1b6aefbc070\""
Oct 23 04:12:42 minikube cri-dockerd[1394]: time="2024-10-23T04:12:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-tsv5l_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9cba9665ccdd3225eeaa4d4cc7086da23b01c0328b510c6213862d47410914a2\""
Oct 23 04:12:42 minikube cri-dockerd[1394]: time="2024-10-23T04:12:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-ps28w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"00a596396f8309bf1f5efe66704be12dd27b74cd49991c12c299c1b6aefbc070\""
Oct 23 04:12:50 minikube cri-dockerd[1394]: time="2024-10-23T04:12:50Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 23 04:13:01 minikube cri-dockerd[1394]: time="2024-10-23T04:13:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/180c7ef353214684bb3bce00347c7dc2cb19a86697052fba6936bba66ff34606/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:13:01 minikube cri-dockerd[1394]: time="2024-10-23T04:13:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4e4b3d2e0321913cc00bcd213896f0325ed7d3d18b3c51288621fe1a9dae756/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:13:05 minikube cri-dockerd[1394]: time="2024-10-23T04:13:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/28a17a6325432fdbb76758eb7921fafc4843896197a15888f93bf80152a569aa/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 23 04:13:05 minikube cri-dockerd[1394]: time="2024-10-23T04:13:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4455d1d1e56a4144cb6659afc1b1e50dd5c1ca684725e9ae78e640c204f0d2e0/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 23 04:13:05 minikube cri-dockerd[1394]: time="2024-10-23T04:13:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c9162d24bb0e84946182b127ff674909475dcc1a8565169f3ceeeea5e5e77d1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 23 04:13:05 minikube cri-dockerd[1394]: time="2024-10-23T04:13:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/026173f0117ad56276811a2f6f4afad85b5a51a6eb4d3b47f327bb94ccb53070/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 23 04:13:07 minikube dockerd[1102]: time="2024-10-23T04:13:07.575729016Z" level=info msg="ignoring event" container=d3248d8b9249bbf8448fd86278043a706ad7894a7a61172c780fd8e12323a319 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 04:13:10 minikube dockerd[1102]: time="2024-10-23T04:13:10.743402885Z" level=info msg="ignoring event" container=b393beb340e6d904fe0007445324e453b9744c75a26a759852e58a8ae5ac9f36 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 04:13:10 minikube dockerd[1102]: time="2024-10-23T04:13:10.900600835Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:13:10 minikube dockerd[1102]: time="2024-10-23T04:13:10.902268296Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:13:29 minikube dockerd[1102]: time="2024-10-23T04:13:29.636000141Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:13:29 minikube dockerd[1102]: time="2024-10-23T04:13:29.636124353Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:13:59 minikube dockerd[1102]: time="2024-10-23T04:13:59.687695044Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:13:59 minikube dockerd[1102]: time="2024-10-23T04:13:59.687792253Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:14:43 minikube dockerd[1102]: time="2024-10-23T04:14:43.003406520Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:14:43 minikube dockerd[1102]: time="2024-10-23T04:14:43.005850663Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:16:13 minikube dockerd[1102]: time="2024-10-23T04:16:13.965856567Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:16:13 minikube dockerd[1102]: time="2024-10-23T04:16:13.965972878Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:18:56 minikube dockerd[1102]: time="2024-10-23T04:18:56.576364425Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:18:56 minikube dockerd[1102]: time="2024-10-23T04:18:56.576520839Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:24:05 minikube dockerd[1102]: time="2024-10-23T04:24:05.605710558Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:24:05 minikube dockerd[1102]: time="2024-10-23T04:24:05.605865072Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:25:48 minikube cri-dockerd[1394]: time="2024-10-23T04:25:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9a8c12438eb0802b8cd08578ff579fc198e00a19f035487afdb803d931bf6cf3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 23 04:25:50 minikube dockerd[1102]: time="2024-10-23T04:25:50.910730879Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:25:50 minikube dockerd[1102]: time="2024-10-23T04:25:50.911028209Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:26:06 minikube dockerd[1102]: time="2024-10-23T04:26:06.363091961Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:26:06 minikube dockerd[1102]: time="2024-10-23T04:26:06.363213472Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:26:34 minikube dockerd[1102]: time="2024-10-23T04:26:34.445338494Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 23 04:26:34 minikube dockerd[1102]: time="2024-10-23T04:26:34.445423000Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 23 04:26:56 minikube dockerd[1102]: time="2024-10-23T04:26:56.805203667Z" level=info msg="ignoring event" container=94b5f9851dde65ff5f38429b198f879db2532e5f22b3f7e99cc897b7016bcf08 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 04:26:57 minikube cri-dockerd[1394]: time="2024-10-23T04:26:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af82719bc81ea252e7d92885ac44b06f4751b5f73a870af4597c7ae64e9e66ce/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 23 04:27:09 minikube cri-dockerd[1394]: time="2024-10-23T04:27:09Z" level=info msg="Pulling image gamberfox/lab5-service:11: a173f2aee8e9: Downloading [======>                                            ]  8.637MB/64.39MB"
Oct 23 04:27:19 minikube cri-dockerd[1394]: time="2024-10-23T04:27:19Z" level=info msg="Pulling image gamberfox/lab5-service:11: cdd62bf39133: Downloading [===================>                               ]  19.79MB/49.56MB"
Oct 23 04:27:29 minikube cri-dockerd[1394]: time="2024-10-23T04:27:29Z" level=info msg="Pulling image gamberfox/lab5-service:11: a173f2aee8e9: Downloading [============================>                      ]  36.72MB/64.39MB"
Oct 23 04:27:39 minikube cri-dockerd[1394]: time="2024-10-23T04:27:39Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Downloading [=>                                                 ]  6.486MB/211.3MB"
Oct 23 04:27:49 minikube cri-dockerd[1394]: time="2024-10-23T04:27:49Z" level=info msg="Pulling image gamberfox/lab5-service:11: a173f2aee8e9: Downloading [===============================================>   ]  61.03MB/64.39MB"
Oct 23 04:27:59 minikube cri-dockerd[1394]: time="2024-10-23T04:27:59Z" level=info msg="Pulling image gamberfox/lab5-service:11: cc48f13b5f0f: Downloading [==============================>                    ]  10.96MB/18.06MB"
Oct 23 04:28:09 minikube cri-dockerd[1394]: time="2024-10-23T04:28:09Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Downloading [=========>                                         ]  39.45MB/211.3MB"
Oct 23 04:28:19 minikube cri-dockerd[1394]: time="2024-10-23T04:28:19Z" level=info msg="Pulling image gamberfox/lab5-service:11: a173f2aee8e9: Extracting [======>                                            ]  7.799MB/64.39MB"
Oct 23 04:28:29 minikube cri-dockerd[1394]: time="2024-10-23T04:28:29Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Downloading [======================>                            ]  95.09MB/211.3MB"
Oct 23 04:28:39 minikube cri-dockerd[1394]: time="2024-10-23T04:28:39Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Downloading [===============================>                   ]  133.5MB/211.3MB"
Oct 23 04:28:49 minikube cri-dockerd[1394]: time="2024-10-23T04:28:49Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Downloading [======================================>            ]  162.1MB/211.3MB"
Oct 23 04:28:59 minikube cri-dockerd[1394]: time="2024-10-23T04:28:59Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Downloading [=============================================>     ]  191.8MB/211.3MB"
Oct 23 04:29:09 minikube cri-dockerd[1394]: time="2024-10-23T04:29:09Z" level=info msg="Pulling image gamberfox/lab5-service:11: 01272fe8adba: Extracting [=====>                                             ]  22.84MB/211.3MB"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
f5ac537168bd7       045733566833c       2 minutes ago       Running             kube-controller-manager     5                   1191245b20c83       kube-controller-manager-minikube
55e8a44d7ec6a       07655ddf2eebe       15 minutes ago      Running             kubernetes-dashboard        5                   4455d1d1e56a4       kubernetes-dashboard-695b96c756-4lnrb
c78c8588d8015       6e38f40d628db       15 minutes ago      Running             storage-provisioner         16                  a4e4b3d2e0321       storage-provisioner
92f478c6d00aa       115053965e86b       16 minutes ago      Running             dashboard-metrics-scraper   3                   28a17a6325432       dashboard-metrics-scraper-c5db448b4-tsv5l
5ff4fcbd46d40       cbb01a7bd410d       16 minutes ago      Running             coredns                     3                   4c9162d24bb0e       coredns-6f6b679f8f-ps28w
b393beb340e6d       07655ddf2eebe       16 minutes ago      Exited              kubernetes-dashboard        4                   4455d1d1e56a4       kubernetes-dashboard-695b96c756-4lnrb
d3248d8b9249b       6e38f40d628db       16 minutes ago      Exited              storage-provisioner         15                  a4e4b3d2e0321       storage-provisioner
6f4e91648d665       ad83b2ca7b09e       16 minutes ago      Running             kube-proxy                  3                   180c7ef353214       kube-proxy-l9ksf
94b5f9851dde6       045733566833c       16 minutes ago      Exited              kube-controller-manager     4                   1191245b20c83       kube-controller-manager-minikube
4594cae8d2e80       604f5db92eaa8       16 minutes ago      Running             kube-apiserver              3                   4c4dd0c8b77b9       kube-apiserver-minikube
fd21e0429775e       2e96e5913fc06       16 minutes ago      Running             etcd                        3                   00a64e8b4dd6d       etcd-minikube
fa6f4e5e21eed       1766f54c897f0       16 minutes ago      Running             kube-scheduler              3                   47e13b0244b4b       kube-scheduler-minikube
b993d3c11e3d7       115053965e86b       2 hours ago         Exited              dashboard-metrics-scraper   2                   9cba9665ccdd3       dashboard-metrics-scraper-c5db448b4-tsv5l
1d6e7bc11b35c       cbb01a7bd410d       2 hours ago         Exited              coredns                     2                   00a596396f830       coredns-6f6b679f8f-ps28w
ed7551dc2c4b6       604f5db92eaa8       2 hours ago         Exited              kube-apiserver              2                   6d01c48e4c962       kube-apiserver-minikube
dcc2c593fd0ee       2e96e5913fc06       2 hours ago         Exited              etcd                        2                   17bc69b2b1b9d       etcd-minikube
bb5f228d5ac85       ad83b2ca7b09e       2 hours ago         Exited              kube-proxy                  2                   c067660cf121b       kube-proxy-l9ksf
29b960ba2a666       1766f54c897f0       2 hours ago         Exited              kube-scheduler              2                   464723406b580       kube-scheduler-minikube


==> coredns [1d6e7bc11b35] <==
[INFO] 127.0.0.1:51823 - 47897 "HINFO IN 7075303551410284997.4957185805314993296. udp 57 false 512" NXDOMAIN qr,rd,ra 57 1.065202895s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1557911461]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Oct-2024 02:19:23.010) (total time: 10836ms):
Trace[1557911461]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10591ms (02:19:33.602)
Trace[1557911461]: [10.83631254s] [10.83631254s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[564762616]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Oct-2024 02:19:22.976) (total time: 11014ms):
Trace[564762616]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10625ms (02:19:33.601)
Trace[564762616]: [11.014759422s] [11.014759422s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1409590752]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Oct-2024 02:19:22.994) (total time: 11027ms):
Trace[1409590752]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10807ms (02:19:33.802)
Trace[1409590752]: [11.027893079s] [11.027893079s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: Trace[343055260]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Oct-2024 02:19:35.073) (total time: 11943ms):
Trace[343055260]: ---"Objects listed" error:<nil> 11943ms (02:19:47.023)
Trace[343055260]: [11.943648294s] [11.943648294s] END
[INFO] plugin/kubernetes: Trace[133621362]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Oct-2024 02:19:35.299) (total time: 11750ms):
Trace[133621362]: ---"Objects listed" error:<nil> 11748ms (02:19:47.055)
Trace[133621362]: [11.750494643s] [11.750494643s] END
[INFO] plugin/kubernetes: Trace[63347505]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Oct-2024 02:19:34.971) (total time: 12290ms):
Trace[63347505]: ---"Objects listed" error:<nil> 12290ms (02:19:47.269)
Trace[63347505]: [12.290499621s] [12.290499621s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.315245715s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.372891674s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.036604055s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.787256727s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.013388278s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.91395287s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.941750135s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.686545689s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.81341895s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.239135116s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.064549741s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.23564878s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.463972314s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.388217934s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.095560855s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.491794074s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.31448215s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 19.185329919s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.87053209s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.038693427s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.155262338s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.234282312s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.644824896s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.641991358s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.466580638s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.595095577s


==> coredns [5ff4fcbd46d4] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:37818 - 47895 "HINFO IN 2309846013610457840.885002646589155811. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.49937989s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_11T13_38_12_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 11 Sep 2024 18:38:04 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 23 Oct 2024 04:29:13 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 23 Oct 2024 04:28:09 +0000   Wed, 11 Sep 2024 18:37:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 23 Oct 2024 04:28:09 +0000   Wed, 11 Sep 2024 18:37:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 23 Oct 2024 04:28:09 +0000   Wed, 11 Sep 2024 18:37:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 23 Oct 2024 04:28:09 +0000   Wed, 23 Oct 2024 00:01:28 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3933828Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3933828Ki
  pods:               110
System Info:
  Machine ID:                 0997baecebe84a44b1979285385798c4
  System UUID:                0997baecebe84a44b1979285385798c4
  Boot ID:                    cacdd012-938b-4702-8f9f-2d114e690745
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     service-c-6bc8b97877-z5jp9                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h5m
  default                     service-c-pod                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m32s
  default                     service-pod                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m22s
  kube-system                 coredns-6f6b679f8f-ps28w                     100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     41d
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         41d
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         41d
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         41d
  kube-system                 kube-proxy-l9ksf                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         41d
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         41d
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         41d
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-tsv5l    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41d
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-4lnrb        0 (0%)        0 (0%)      0 (0%)           0 (0%)         41d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           16m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  16m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           16m                kubelet          Starting kubelet.
  Warning  CgroupV1                           16m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            16m (x7 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              16m (x7 over 16m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               16m (x7 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            16m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     16m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     2m16s              node-controller  Node minikube event: Registered Node minikube in Controller


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    environment=testing
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=false
                    minikube.k8s.io/updated_at=2024_09_11T14_11_15_0700
                    minikube.k8s.io/version=v1.34.0
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 11 Sep 2024 19:11:13 +0000
Taints:             environment=testing:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Wed, 23 Oct 2024 04:29:08 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 23 Oct 2024 04:28:59 +0000   Wed, 23 Oct 2024 04:13:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 23 Oct 2024 04:28:59 +0000   Wed, 23 Oct 2024 04:13:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 23 Oct 2024 04:28:59 +0000   Wed, 23 Oct 2024 04:13:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 23 Oct 2024 04:28:59 +0000   Wed, 23 Oct 2024 04:13:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.3
  Hostname:    minikube-m02
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3933828Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3933828Ki
  pods:               110
System Info:
  Machine ID:                 37db55d01170476eb31497e395414c8d
  System UUID:                37db55d01170476eb31497e395414c8d
  Boot ID:                    cacdd012-938b-4702-8f9f-2d114e690745
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (1 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kube-proxy-v7jrx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests  Limits
  --------           --------  ------
  cpu                0 (0%)    0 (0%)
  memory             0 (0%)    0 (0%)
  ephemeral-storage  0 (0%)    0 (0%)
  hugepages-1Gi      0 (0%)    0 (0%)
  hugepages-2Mi      0 (0%)    0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           15m                kube-proxy       
  Normal   RegisteredNode                     16m                node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  16m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           16m                kubelet          Starting kubelet.
  Warning  CgroupV1                           16m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            16m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            15m (x7 over 16m)  kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              15m (x7 over 16m)  kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               15m (x7 over 16m)  kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal   NodeNotReady                       15m                node-controller  Node minikube-m02 status is now: NodeNotReady
  Normal   RegisteredNode                     2m17s              node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    environment=production
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=false
                    minikube.k8s.io/updated_at=2024_09_11T14_43_05_0700
                    minikube.k8s.io/version=v1.34.0
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 11 Sep 2024 19:43:03 +0000
Taints:             environment=production:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Wed, 23 Oct 2024 04:29:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 23 Oct 2024 04:24:23 +0000   Wed, 23 Oct 2024 04:14:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 23 Oct 2024 04:24:23 +0000   Wed, 23 Oct 2024 04:14:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 23 Oct 2024 04:24:23 +0000   Wed, 23 Oct 2024 04:14:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 23 Oct 2024 04:24:23 +0000   Wed, 23 Oct 2024 04:14:06 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.4
  Hostname:    minikube-m03
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3933828Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3933828Ki
  pods:               110
System Info:
  Machine ID:                 87044a5bd80c44a6b096a3709c02afcf
  System UUID:                87044a5bd80c44a6b096a3709c02afcf
  Boot ID:                    cacdd012-938b-4702-8f9f-2d114e690745
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (1 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kube-proxy-qgd82    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests  Limits
  --------           --------  ------
  cpu                0 (0%)    0 (0%)
  memory             0 (0%)    0 (0%)
  ephemeral-storage  0 (0%)    0 (0%)
  hugepages-1Gi      0 (0%)    0 (0%)
  hugepages-2Mi      0 (0%)    0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           15m                kube-proxy       
  Normal   RegisteredNode                     16m                node-controller  Node minikube-m03 event: Registered Node minikube-m03 in Controller
  Normal   NodeNotReady                       15m                node-controller  Node minikube-m03 status is now: NodeNotReady
  Warning  PossibleMemoryBackedVolumesOnDisk  15m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           15m                kubelet          Starting kubelet.
  Warning  CgroupV1                           15m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            15m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            15m (x7 over 15m)  kubelet          Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              15m (x7 over 15m)  kubelet          Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               15m (x7 over 15m)  kubelet          Node minikube-m03 status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     2m17s              node-controller  Node minikube-m03 event: Registered Node minikube-m03 in Controller


==> dmesg <==
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000091]  #2 #3
[  +0.060591] PCI: Fatal: No config space access function found
[  +0.049389] PCI: System does not support PCI
[  +0.074664] kvm: no hardware support
[  +0.000007] kvm: no hardware support
[  +9.154800] FS-Cache: Duplicate cookie detected
[  +0.060663] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.064143] FS-Cache: O-cookie d=000000002b38caf8{9P.session} n=00000000481955fb
[  +0.010517] FS-Cache: O-key=[10] '34323934393338323331'
[  +0.005757] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.001602] FS-Cache: N-cookie d=000000002b38caf8{9P.session} n=0000000054cd4d58
[  +0.001775] FS-Cache: N-key=[10] '34323934393338323331'
[  +2.375055] FS-Cache: Duplicate cookie detected
[  +0.005362] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.001563] FS-Cache: O-cookie d=000000002b38caf8{9P.session} n=0000000065ae3efe
[  +0.001522] FS-Cache: O-key=[10] '34323934393338343833'
[  +0.001173] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.001209] FS-Cache: N-cookie d=000000002b38caf8{9P.session} n=000000000c965132
[  +0.001086] FS-Cache: N-key=[10] '34323934393338343833'
[  +0.093244] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000008]  failed 2
[  +0.023581] FS-Cache: Duplicate cookie detected
[  +0.000903] FS-Cache: O-cookie c=0000000d [p=00000002 fl=222 nc=0 na=1]
[  +0.000852] FS-Cache: O-cookie d=000000002b38caf8{9P.session} n=00000000e93a4412
[  +0.001015] FS-Cache: O-key=[10] '34323934393338343936'
[  +0.000947] FS-Cache: N-cookie c=0000000e [p=00000002 fl=2 nc=0 na=1]
[  +0.012710] FS-Cache: N-cookie d=000000002b38caf8{9P.session} n=00000000b21a7a21
[  +0.003283] FS-Cache: N-key=[10] '34323934393338343936'
[  +0.014539] WSL (1) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +1.536710] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[ +11.605833] Exception: 
[  +0.000007] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +0.002218] WSL (1) ERROR: InitEntryUtilityVm:2311: Init has exited. Terminating distribution
[  +0.561942] FS-Cache: Duplicate cookie detected
[  +0.001263] FS-Cache: O-cookie c=00000017 [p=00000002 fl=222 nc=0 na=1]
[  +0.001150] FS-Cache: O-cookie d=000000002b38caf8{9P.session} n=000000008b360960
[  +0.001218] FS-Cache: O-key=[10] '34323934393339383730'
[  +0.000865] FS-Cache: N-cookie c=00000018 [p=00000002 fl=2 nc=0 na=1]
[  +0.000964] FS-Cache: N-cookie d=000000002b38caf8{9P.session} n=00000000bb33f1ed
[  +0.000987] FS-Cache: N-key=[10] '34323934393339383730'
[  +0.119471] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.031163] FS-Cache: Duplicate cookie detected
[  +0.000821] FS-Cache: O-cookie c=00000019 [p=00000002 fl=222 nc=0 na=1]
[  +0.001708] FS-Cache: O-cookie d=000000002b38caf8{9P.session} n=00000000e93a4412
[  +0.002380] FS-Cache: O-key=[10] '34323934393339383836'
[  +0.001629] FS-Cache: N-cookie c=0000001a [p=00000002 fl=2 nc=0 na=1]
[  +0.006733] FS-Cache: N-cookie d=000000002b38caf8{9P.session} n=0000000092e10fbc
[  +0.002090] FS-Cache: N-key=[10] '34323934393339383836'
[  +0.010084] WSL (1) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.473502] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +1.841106] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +4.227003] EXT4-fs (sdd): warning: mounting fs with errors, running e2fsck is recommended
[  +1.572469] netlink: 'init': attribute type 4 has an invalid length.
[Oct23 04:05] hrtimer: interrupt took 5142102 ns
[Oct23 04:12] tmpfs: Unknown parameter 'noswap'
[Oct23 04:13] tmpfs: Unknown parameter 'noswap'
[ +32.080135] tmpfs: Unknown parameter 'noswap'


==> etcd [dcc2c593fd0e] <==
{"level":"info","ts":"2024-10-23T03:28:13.849288Z","caller":"traceutil/trace.go:171","msg":"trace[1901434268] transaction","detail":"{read_only:false; response_revision:28972; number_of_response:1; }","duration":"231.357803ms","start":"2024-10-23T03:28:13.617904Z","end":"2024-10-23T03:28:13.849262Z","steps":["trace[1901434268] 'process raft request'  (duration: 231.188551ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:14.111492Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"155.637532ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:14.111578Z","caller":"traceutil/trace.go:171","msg":"trace[2110084469] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:28972; }","duration":"155.741363ms","start":"2024-10-23T03:28:13.955816Z","end":"2024-10-23T03:28:14.111557Z","steps":["trace[2110084469] 'range keys from in-memory index tree'  (duration: 155.617626ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T03:28:14.216104Z","caller":"traceutil/trace.go:171","msg":"trace[502113020] linearizableReadLoop","detail":"{readStateIndex:35051; appliedIndex:35050; }","duration":"597.268757ms","start":"2024-10-23T03:28:13.618814Z","end":"2024-10-23T03:28:14.216083Z","steps":["trace[502113020] 'read index received'  (duration: 231.193152ms)","trace[502113020] 'applied index is now lower than readState.Index'  (duration: 366.074704ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T03:28:14.216205Z","caller":"traceutil/trace.go:171","msg":"trace[2096615779] transaction","detail":"{read_only:false; response_revision:28974; number_of_response:1; }","duration":"476.355697ms","start":"2024-10-23T03:28:13.739838Z","end":"2024-10-23T03:28:14.216194Z","steps":["trace[2096615779] 'process raft request'  (duration: 476.199649ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T03:28:14.216205Z","caller":"traceutil/trace.go:171","msg":"trace[1066775256] transaction","detail":"{read_only:false; response_revision:28973; number_of_response:1; }","duration":"597.481722ms","start":"2024-10-23T03:28:13.618696Z","end":"2024-10-23T03:28:14.216178Z","steps":["trace[1066775256] 'process raft request'  (duration: 597.219942ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:14.216307Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:13.739792Z","time spent":"476.444724ms","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":537,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube-m03\" mod_revision:28965 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube-m03\" value_size:484 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube-m03\" > >"}
{"level":"warn","ts":"2024-10-23T03:28:14.216392Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:13.618668Z","time spent":"597.641772ms","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":537,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube-m02\" mod_revision:28964 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube-m02\" value_size:484 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube-m02\" > >"}
{"level":"warn","ts":"2024-10-23T03:28:14.226113Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"607.180103ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-10-23T03:28:14.226995Z","caller":"traceutil/trace.go:171","msg":"trace[1066344846] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:28974; }","duration":"607.715568ms","start":"2024-10-23T03:28:13.618809Z","end":"2024-10-23T03:28:14.226525Z","steps":["trace[1066344846] 'agreement among raft nodes before linearized reading'  (duration: 597.47362ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:14.256973Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:13.618775Z","time spent":"638.157123ms","remote":"127.0.0.1:43124","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-10-23T03:28:14.451279Z","caller":"traceutil/trace.go:171","msg":"trace[184309307] transaction","detail":"{read_only:false; response_revision:28976; number_of_response:1; }","duration":"165.866575ms","start":"2024-10-23T03:28:14.285386Z","end":"2024-10-23T03:28:14.451252Z","steps":["trace[184309307] 'process raft request'  (duration: 165.794953ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T03:28:14.451482Z","caller":"traceutil/trace.go:171","msg":"trace[882032874] transaction","detail":"{read_only:false; response_revision:28975; number_of_response:1; }","duration":"449.131531ms","start":"2024-10-23T03:28:14.002323Z","end":"2024-10-23T03:28:14.451454Z","steps":["trace[882032874] 'process raft request'  (duration: 368.793341ms)","trace[882032874] 'compare'  (duration: 79.889752ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T03:28:14.451594Z","caller":"traceutil/trace.go:171","msg":"trace[1095550488] linearizableReadLoop","detail":"{readStateIndex:35053; appliedIndex:35052; }","duration":"235.407047ms","start":"2024-10-23T03:28:14.216168Z","end":"2024-10-23T03:28:14.451575Z","steps":["trace[1095550488] 'read index received'  (duration: 154.966926ms)","trace[1095550488] 'applied index is now lower than readState.Index'  (duration: 80.43842ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T03:28:14.451629Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:14.002288Z","time spent":"449.253468ms","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:28966 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-10-23T03:28:14.451963Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"340.317989ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:14.452010Z","caller":"traceutil/trace.go:171","msg":"trace[1503019158] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:28976; }","duration":"340.373306ms","start":"2024-10-23T03:28:14.111623Z","end":"2024-10-23T03:28:14.451997Z","steps":["trace[1503019158] 'agreement among raft nodes before linearized reading'  (duration: 340.298083ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:14.452322Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"481.263905ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:14.452377Z","caller":"traceutil/trace.go:171","msg":"trace[1103495014] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28976; }","duration":"481.324124ms","start":"2024-10-23T03:28:13.971036Z","end":"2024-10-23T03:28:14.452360Z","steps":["trace[1103495014] 'agreement among raft nodes before linearized reading'  (duration: 481.229695ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:14.452427Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:13.970970Z","time spent":"481.446462ms","remote":"127.0.0.1:54412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-23T03:28:14.452795Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"712.507773ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/production-pod7\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:14.452846Z","caller":"traceutil/trace.go:171","msg":"trace[1838968019] range","detail":"{range_begin:/registry/pods/default/production-pod7; range_end:; response_count:0; response_revision:28976; }","duration":"712.560889ms","start":"2024-10-23T03:28:13.740272Z","end":"2024-10-23T03:28:14.452832Z","steps":["trace[1838968019] 'agreement among raft nodes before linearized reading'  (duration: 712.472662ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:14.452896Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:13.740221Z","time spent":"712.666221ms","remote":"127.0.0.1:43134","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":0,"response size":29,"request content":"key:\"/registry/pods/default/production-pod7\" "}
{"level":"warn","ts":"2024-10-23T03:28:14.453155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"203.72211ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-10-23T03:28:14.453195Z","caller":"traceutil/trace.go:171","msg":"trace[1464922987] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:28976; }","duration":"203.774726ms","start":"2024-10-23T03:28:14.249410Z","end":"2024-10-23T03:28:14.453185Z","steps":["trace[1464922987] 'agreement among raft nodes before linearized reading'  (duration: 203.701303ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T03:28:20.835925Z","caller":"traceutil/trace.go:171","msg":"trace[1852626212] transaction","detail":"{read_only:false; response_revision:28980; number_of_response:1; }","duration":"131.09939ms","start":"2024-10-23T03:28:20.704790Z","end":"2024-10-23T03:28:20.835890Z","steps":["trace[1852626212] 'process raft request'  (duration: 62.565228ms)","trace[1852626212] 'compare'  (duration: 68.36361ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T03:28:20.905667Z","caller":"traceutil/trace.go:171","msg":"trace[1982978243] transaction","detail":"{read_only:false; response_revision:28981; number_of_response:1; }","duration":"167.466367ms","start":"2024-10-23T03:28:20.738159Z","end":"2024-10-23T03:28:20.905625Z","steps":["trace[1982978243] 'process raft request'  (duration: 166.993922ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:21.092272Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.923729ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:21.092716Z","caller":"traceutil/trace.go:171","msg":"trace[1342424252] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:28981; }","duration":"130.370366ms","start":"2024-10-23T03:28:20.962311Z","end":"2024-10-23T03:28:21.092681Z","steps":["trace[1342424252] 'range keys from in-memory index tree'  (duration: 129.897221ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.296912Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.986551757s","expected-duration":"100ms","prefix":"","request":"header:<ID:3238530922460078345 > lease_revoke:<id:2cf192b72b59e0be>","response":"size:29"}
{"level":"info","ts":"2024-10-23T03:28:28.297236Z","caller":"traceutil/trace.go:171","msg":"trace[1132824591] linearizableReadLoop","detail":"{readStateIndex:35063; appliedIndex:35062; }","duration":"2.097209679s","start":"2024-10-23T03:28:26.200005Z","end":"2024-10-23T03:28:28.297215Z","steps":["trace[1132824591] 'read index received'  (duration: 110.278405ms)","trace[1132824591] 'applied index is now lower than readState.Index'  (duration: 1.986929674s)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T03:28:28.297646Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.021026286s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:28.297725Z","caller":"traceutil/trace.go:171","msg":"trace[296711329] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:28982; }","duration":"1.021117414s","start":"2024-10-23T03:28:27.276589Z","end":"2024-10-23T03:28:28.297707Z","steps":["trace[296711329] 'range keys from in-memory index tree'  (duration: 1.02100888s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.298356Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.098327025s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:28.298434Z","caller":"traceutil/trace.go:171","msg":"trace[247517263] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations/; range_end:/registry/mutatingwebhookconfigurations0; response_count:0; response_revision:28982; }","duration":"2.098415153s","start":"2024-10-23T03:28:26.199996Z","end":"2024-10-23T03:28:28.298411Z","steps":["trace[247517263] 'agreement among raft nodes before linearized reading'  (duration: 2.098284913s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.298533Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:26.199935Z","time spent":"2.098546994s","remote":"127.0.0.1:48480","response type":"/etcdserverpb.KV/Range","request count":0,"request size":86,"response count":0,"response size":29,"request content":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true "}
{"level":"info","ts":"2024-10-23T03:28:28.319757Z","caller":"traceutil/trace.go:171","msg":"trace[1605366763] transaction","detail":"{read_only:false; response_revision:28983; number_of_response:1; }","duration":"1.861101447s","start":"2024-10-23T03:28:26.458562Z","end":"2024-10-23T03:28:28.319664Z","steps":["trace[1605366763] 'process raft request'  (duration: 1.838535648s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.320791Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:26.458530Z","time spent":"1.862161576s","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:28972 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-10-23T03:28:28.380748Z","caller":"traceutil/trace.go:171","msg":"trace[1936114833] transaction","detail":"{read_only:false; response_revision:28984; number_of_response:1; }","duration":"1.921447063s","start":"2024-10-23T03:28:26.459269Z","end":"2024-10-23T03:28:28.380716Z","steps":["trace[1936114833] 'process raft request'  (duration: 1.860375121s)","trace[1936114833] 'compare'  (duration: 60.513669ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T03:28:28.380988Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:26.459236Z","time spent":"1.921605713s","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:28975 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-10-23T03:28:28.381715Z","caller":"traceutil/trace.go:171","msg":"trace[699288512] transaction","detail":"{read_only:false; response_revision:28985; number_of_response:1; }","duration":"1.858718508s","start":"2024-10-23T03:28:26.522970Z","end":"2024-10-23T03:28:28.381688Z","steps":["trace[699288512] 'process raft request'  (duration: 1.857674084s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.381977Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:26.522952Z","time spent":"1.858894662s","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":537,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube-m02\" mod_revision:28973 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube-m02\" value_size:484 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube-m02\" > >"}
{"level":"info","ts":"2024-10-23T03:28:28.383325Z","caller":"traceutil/trace.go:171","msg":"trace[1543489572] transaction","detail":"{read_only:false; response_revision:28986; number_of_response:1; }","duration":"1.133627211s","start":"2024-10-23T03:28:27.249672Z","end":"2024-10-23T03:28:28.383299Z","steps":["trace[1543489572] 'process raft request'  (duration: 1.131930885s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.383481Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:27.249639Z","time spent":"1.133747548s","remote":"127.0.0.1:43230","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":537,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube-m03\" mod_revision:28974 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube-m03\" value_size:484 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube-m03\" > >"}
{"level":"warn","ts":"2024-10-23T03:28:28.383727Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"716.157426ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:28.383783Z","caller":"traceutil/trace.go:171","msg":"trace[1460047861] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28986; }","duration":"716.212343ms","start":"2024-10-23T03:28:27.667552Z","end":"2024-10-23T03:28:28.383764Z","steps":["trace[1460047861] 'agreement among raft nodes before linearized reading'  (duration: 716.117814ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.383834Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:27.667464Z","time spent":"716.358489ms","remote":"127.0.0.1:54412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-23T03:28:28.384118Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"801.438377ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:28.384165Z","caller":"traceutil/trace.go:171","msg":"trace[2060900152] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28986; }","duration":"801.487293ms","start":"2024-10-23T03:28:27.582664Z","end":"2024-10-23T03:28:28.384151Z","steps":["trace[2060900152] 'agreement among raft nodes before linearized reading'  (duration: 801.398565ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.384205Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:27.582589Z","time spent":"801.60653ms","remote":"127.0.0.1:54420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-23T03:28:28.384415Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.134483676s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/production-pod7\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T03:28:28.384456Z","caller":"traceutil/trace.go:171","msg":"trace[1446093998] range","detail":"{range_begin:/registry/pods/default/production-pod7; range_end:; response_count:0; response_revision:28986; }","duration":"1.13452969s","start":"2024-10-23T03:28:27.249916Z","end":"2024-10-23T03:28:28.384446Z","steps":["trace[1446093998] 'agreement among raft nodes before linearized reading'  (duration: 1.134454667s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.384495Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:27.249868Z","time spent":"1.134617117s","remote":"127.0.0.1:43134","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":0,"response size":29,"request content":"key:\"/registry/pods/default/production-pod7\" "}
{"level":"warn","ts":"2024-10-23T03:28:28.384739Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.862018832s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-10-23T03:28:28.384783Z","caller":"traceutil/trace.go:171","msg":"trace[793528248] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:28986; }","duration":"1.862062545s","start":"2024-10-23T03:28:26.522705Z","end":"2024-10-23T03:28:28.384768Z","steps":["trace[793528248] 'agreement among raft nodes before linearized reading'  (duration: 1.861977219s)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T03:28:28.384822Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T03:28:26.522650Z","time spent":"1.862163376s","remote":"127.0.0.1:43124","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-10-23T03:28:28.707648Z","caller":"traceutil/trace.go:171","msg":"trace[514029716] transaction","detail":"{read_only:false; response_revision:28987; number_of_response:1; }","duration":"158.563781ms","start":"2024-10-23T03:28:28.549043Z","end":"2024-10-23T03:28:28.707606Z","steps":["trace[514029716] 'process raft request'  (duration: 109.609097ms)","trace[514029716] 'compare'  (duration: 17.642772ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T03:28:28.736402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"186.688104ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-10-23T03:28:28.736693Z","caller":"traceutil/trace.go:171","msg":"trace[963837401] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:28987; }","duration":"186.9968ms","start":"2024-10-23T03:28:28.549668Z","end":"2024-10-23T03:28:28.736665Z","steps":["trace[963837401] 'agreement among raft nodes before linearized reading'  (duration: 159.164467ms)","trace[963837401] 'count revisions from in-memory index tree'  (duration: 27.482424ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T03:28:28.738370Z","caller":"traceutil/trace.go:171","msg":"trace[1641370363] linearizableReadLoop","detail":"{readStateIndex:35068; appliedIndex:35067; }","duration":"126.720505ms","start":"2024-10-23T03:28:28.549704Z","end":"2024-10-23T03:28:28.676424Z","steps":["trace[1641370363] 'read index received'  (duration: 109.000108ms)","trace[1641370363] 'applied index is now lower than readState.Index'  (duration: 17.718596ms)"],"step_count":2}


==> etcd [fd21e0429775] <==
{"level":"info","ts":"2024-10-23T04:19:28.713509Z","caller":"traceutil/trace.go:171","msg":"trace[1348671284] transaction","detail":"{read_only:false; response_revision:29604; number_of_response:1; }","duration":"135.764786ms","start":"2024-10-23T04:19:28.577713Z","end":"2024-10-23T04:19:28.713478Z","steps":["trace[1348671284] 'process raft request'  (duration: 49.023801ms)","trace[1348671284] 'compare'  (duration: 86.332349ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T04:19:28.713917Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.793494ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T04:19:28.713996Z","caller":"traceutil/trace.go:171","msg":"trace[1388490792] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29604; }","duration":"115.8657ms","start":"2024-10-23T04:19:28.598100Z","end":"2024-10-23T04:19:28.713965Z","steps":["trace[1388490792] 'agreement among raft nodes before linearized reading'  (duration: 115.768691ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:19:35.688913Z","caller":"traceutil/trace.go:171","msg":"trace[627958360] transaction","detail":"{read_only:false; response_revision:29610; number_of_response:1; }","duration":"238.442585ms","start":"2024-10-23T04:19:35.450438Z","end":"2024-10-23T04:19:35.688881Z","steps":["trace[627958360] 'process raft request'  (duration: 238.035252ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:20:01.586809Z","caller":"traceutil/trace.go:171","msg":"trace[717455189] transaction","detail":"{read_only:false; response_revision:29637; number_of_response:1; }","duration":"118.965399ms","start":"2024-10-23T04:20:01.467823Z","end":"2024-10-23T04:20:01.586789Z","steps":["trace[717455189] 'process raft request'  (duration: 30.234968ms)","trace[717455189] 'compare'  (duration: 88.459607ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T04:20:04.435362Z","caller":"traceutil/trace.go:171","msg":"trace[518197415] transaction","detail":"{read_only:false; response_revision:29639; number_of_response:1; }","duration":"141.128255ms","start":"2024-10-23T04:20:04.294197Z","end":"2024-10-23T04:20:04.435325Z","steps":["trace[518197415] 'process raft request'  (duration: 140.811427ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:20:19.647662Z","caller":"traceutil/trace.go:171","msg":"trace[1330429847] transaction","detail":"{read_only:false; response_revision:29653; number_of_response:1; }","duration":"107.430926ms","start":"2024-10-23T04:20:19.540202Z","end":"2024-10-23T04:20:19.647633Z","steps":["trace[1330429847] 'process raft request'  (duration: 107.093395ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:20:31.583076Z","caller":"traceutil/trace.go:171","msg":"trace[2049061448] transaction","detail":"{read_only:false; response_revision:29667; number_of_response:1; }","duration":"113.484031ms","start":"2024-10-23T04:20:31.469565Z","end":"2024-10-23T04:20:31.583049Z","steps":["trace[2049061448] 'process raft request'  (duration: 29.542141ms)","trace[2049061448] 'compare'  (duration: 83.72257ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T04:20:37.835739Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.976877ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T04:20:37.835865Z","caller":"traceutil/trace.go:171","msg":"trace[212160100] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29671; }","duration":"165.108989ms","start":"2024-10-23T04:20:37.670735Z","end":"2024-10-23T04:20:37.835844Z","steps":["trace[212160100] 'range keys from in-memory index tree'  (duration: 164.909771ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:20:53.590788Z","caller":"traceutil/trace.go:171","msg":"trace[202754007] transaction","detail":"{read_only:false; response_revision:29688; number_of_response:1; }","duration":"104.555348ms","start":"2024-10-23T04:20:53.486203Z","end":"2024-10-23T04:20:53.590758Z","steps":["trace[202754007] 'process raft request'  (duration: 104.365631ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:21:01.846870Z","caller":"traceutil/trace.go:171","msg":"trace[966018732] transaction","detail":"{read_only:false; response_revision:29697; number_of_response:1; }","duration":"113.036315ms","start":"2024-10-23T04:21:01.733813Z","end":"2024-10-23T04:21:01.846850Z","steps":["trace[966018732] 'process raft request'  (duration: 53.870316ms)","trace[966018732] 'compare'  (duration: 59.045188ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T04:21:06.814392Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.555158ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238530924211003028 > lease_revoke:<id:2cf192b793b6de40>","response":"size:29"}
{"level":"info","ts":"2024-10-23T04:21:11.597203Z","caller":"traceutil/trace.go:171","msg":"trace[1566255563] transaction","detail":"{read_only:false; response_revision:29704; number_of_response:1; }","duration":"118.066721ms","start":"2024-10-23T04:21:11.479067Z","end":"2024-10-23T04:21:11.597134Z","steps":["trace[1566255563] 'process raft request'  (duration: 29.439899ms)","trace[1566255563] 'compare'  (duration: 88.102776ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T04:21:11.598076Z","caller":"traceutil/trace.go:171","msg":"trace[324176054] transaction","detail":"{read_only:false; response_revision:29705; number_of_response:1; }","duration":"117.200445ms","start":"2024-10-23T04:21:11.480848Z","end":"2024-10-23T04:21:11.598049Z","steps":["trace[324176054] 'process raft request'  (duration: 116.79821ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:21:12.274045Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.085334ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T04:21:12.274136Z","caller":"traceutil/trace.go:171","msg":"trace[885801837] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:29707; }","duration":"103.191343ms","start":"2024-10-23T04:21:12.170925Z","end":"2024-10-23T04:21:12.274116Z","steps":["trace[885801837] 'count revisions from in-memory index tree'  (duration: 103.004927ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:21:22.246181Z","caller":"traceutil/trace.go:171","msg":"trace[719347662] transaction","detail":"{read_only:false; response_revision:29716; number_of_response:1; }","duration":"130.313372ms","start":"2024-10-23T04:21:22.115839Z","end":"2024-10-23T04:21:22.246153Z","steps":["trace[719347662] 'process raft request'  (duration: 130.117054ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:21:36.845146Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.57199ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238530924211003186 > lease_revoke:<id:2cf192b793b6dedb>","response":"size:29"}
{"level":"info","ts":"2024-10-23T04:22:45.884438Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29497}
{"level":"info","ts":"2024-10-23T04:22:46.200005Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":29497,"took":"306.259703ms","hash":949395943,"current-db-size-bytes":4014080,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2306048,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-10-23T04:22:46.200795Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":949395943,"revision":29497,"compact-revision":28475}
{"level":"warn","ts":"2024-10-23T04:22:56.159070Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238530924211003593,"retry-timeout":"500ms"}
{"level":"info","ts":"2024-10-23T04:22:56.352389Z","caller":"traceutil/trace.go:171","msg":"trace[660801475] linearizableReadLoop","detail":"{readStateIndex:36025; appliedIndex:36024; }","duration":"693.955467ms","start":"2024-10-23T04:22:55.658405Z","end":"2024-10-23T04:22:56.352360Z","steps":["trace[660801475] 'read index received'  (duration: 693.629838ms)","trace[660801475] 'applied index is now lower than readState.Index'  (duration: 324.229µs)"],"step_count":2}
{"level":"info","ts":"2024-10-23T04:22:56.354108Z","caller":"traceutil/trace.go:171","msg":"trace[556469043] transaction","detail":"{read_only:false; response_revision:29809; number_of_response:1; }","duration":"956.153093ms","start":"2024-10-23T04:22:55.396694Z","end":"2024-10-23T04:22:56.352847Z","steps":["trace[556469043] 'process raft request'  (duration: 955.430829ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:22:56.354342Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T04:22:55.396671Z","time spent":"957.529917ms","remote":"127.0.0.1:33754","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:29805 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-10-23T04:22:56.355426Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"697.000839ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T04:22:56.355537Z","caller":"traceutil/trace.go:171","msg":"trace[1375920659] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29809; }","duration":"697.111649ms","start":"2024-10-23T04:22:55.658397Z","end":"2024-10-23T04:22:56.355509Z","steps":["trace[1375920659] 'agreement among raft nodes before linearized reading'  (duration: 696.980737ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:24:25.773411Z","caller":"traceutil/trace.go:171","msg":"trace[58617462] transaction","detail":"{read_only:false; response_revision:29901; number_of_response:1; }","duration":"218.091185ms","start":"2024-10-23T04:24:25.555212Z","end":"2024-10-23T04:24:25.773303Z","steps":["trace[58617462] 'process raft request'  (duration: 217.796259ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:24:25.812128Z","caller":"traceutil/trace.go:171","msg":"trace[1277962807] linearizableReadLoop","detail":"{readStateIndex:36137; appliedIndex:36135; }","duration":"228.77704ms","start":"2024-10-23T04:24:25.583310Z","end":"2024-10-23T04:24:25.812087Z","steps":["trace[1277962807] 'read index received'  (duration: 189.952372ms)","trace[1277962807] 'applied index is now lower than readState.Index'  (duration: 38.822968ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T04:24:25.812267Z","caller":"traceutil/trace.go:171","msg":"trace[998822477] transaction","detail":"{read_only:false; response_revision:29902; number_of_response:1; }","duration":"237.292801ms","start":"2024-10-23T04:24:25.574951Z","end":"2024-10-23T04:24:25.812244Z","steps":["trace[998822477] 'process raft request'  (duration: 221.785316ms)","trace[998822477] 'compare'  (duration: 15.137752ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T04:24:25.812318Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"228.98946ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T04:24:25.812360Z","caller":"traceutil/trace.go:171","msg":"trace[1764261390] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29902; }","duration":"229.046265ms","start":"2024-10-23T04:24:25.583301Z","end":"2024-10-23T04:24:25.812347Z","steps":["trace[1764261390] 'agreement among raft nodes before linearized reading'  (duration: 228.959357ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:24:25.812452Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.005374ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-23T04:24:25.812507Z","caller":"traceutil/trace.go:171","msg":"trace[290558233] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29902; }","duration":"162.068381ms","start":"2024-10-23T04:24:25.650423Z","end":"2024-10-23T04:24:25.812492Z","steps":["trace[290558233] 'agreement among raft nodes before linearized reading'  (duration: 161.985273ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:25:47.359220Z","caller":"traceutil/trace.go:171","msg":"trace[1348793613] transaction","detail":"{read_only:false; response_revision:29988; number_of_response:1; }","duration":"115.481727ms","start":"2024-10-23T04:25:47.243708Z","end":"2024-10-23T04:25:47.359190Z","steps":["trace[1348793613] 'process raft request'  (duration: 81.877844ms)","trace[1348793613] 'compare'  (duration: 33.146037ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T04:25:51.805861Z","caller":"traceutil/trace.go:171","msg":"trace[1210053707] transaction","detail":"{read_only:false; response_revision:29998; number_of_response:1; }","duration":"100.336902ms","start":"2024-10-23T04:25:51.705498Z","end":"2024-10-23T04:25:51.805835Z","steps":["trace[1210053707] 'process raft request'  (duration: 100.11708ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:26:56.483058Z","caller":"traceutil/trace.go:171","msg":"trace[383817014] transaction","detail":"{read_only:false; response_revision:30081; number_of_response:1; }","duration":"135.928537ms","start":"2024-10-23T04:26:56.347106Z","end":"2024-10-23T04:26:56.483034Z","steps":["trace[383817014] 'process raft request'  (duration: 133.69724ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:27:40.252717Z","caller":"traceutil/trace.go:171","msg":"trace[1179410449] transaction","detail":"{read_only:false; response_revision:30141; number_of_response:1; }","duration":"231.636794ms","start":"2024-10-23T04:27:40.021056Z","end":"2024-10-23T04:27:40.252693Z","steps":["trace[1179410449] 'process raft request'  (duration: 231.508983ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:27:46.142650Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29799}
{"level":"info","ts":"2024-10-23T04:27:46.153815Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":29799,"took":"8.754672ms","hash":4257899934,"current-db-size-bytes":4014080,"current-db-size":"4.0 MB","current-db-size-in-use-bytes":2097152,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-10-23T04:27:46.153895Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4257899934,"revision":29799,"compact-revision":29497}
{"level":"warn","ts":"2024-10-23T04:27:46.205289Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.291693ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238530924211005290 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:29799 > ","response":"size:6"}
{"level":"info","ts":"2024-10-23T04:27:46.206132Z","caller":"traceutil/trace.go:171","msg":"trace[156205720] transaction","detail":"{read_only:false; response_revision:30148; number_of_response:1; }","duration":"122.174069ms","start":"2024-10-23T04:27:46.083889Z","end":"2024-10-23T04:27:46.206063Z","steps":["trace[156205720] 'process raft request'  (duration: 121.592718ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:27:46.209211Z","caller":"traceutil/trace.go:171","msg":"trace[1384265590] compact","detail":"{revision:29799; response_revision:30147; }","duration":"276.744994ms","start":"2024-10-23T04:27:45.929114Z","end":"2024-10-23T04:27:46.205859Z","steps":["trace[1384265590] 'process raft request'  (duration: 100.471956ms)","trace[1384265590] 'check and update compact revision'  (duration: 104.129779ms)"],"step_count":2}
{"level":"info","ts":"2024-10-23T04:28:06.917491Z","caller":"traceutil/trace.go:171","msg":"trace[497177443] transaction","detail":"{read_only:false; response_revision:30169; number_of_response:1; }","duration":"105.214831ms","start":"2024-10-23T04:28:06.812248Z","end":"2024-10-23T04:28:06.917463Z","steps":["trace[497177443] 'process raft request'  (duration: 105.036516ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:28:13.464717Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.027195ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-10-23T04:28:13.469253Z","caller":"traceutil/trace.go:171","msg":"trace[810020946] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:30178; }","duration":"139.573611ms","start":"2024-10-23T04:28:13.329647Z","end":"2024-10-23T04:28:13.469221Z","steps":["trace[810020946] 'count revisions from in-memory index tree'  (duration: 134.710673ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:28:29.650466Z","caller":"traceutil/trace.go:171","msg":"trace[239405233] transaction","detail":"{read_only:false; response_revision:30192; number_of_response:1; }","duration":"132.632263ms","start":"2024-10-23T04:28:29.517397Z","end":"2024-10-23T04:28:29.650030Z","steps":["trace[239405233] 'process raft request'  (duration: 128.420189ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:28:31.877394Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"137.433388ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-10-23T04:28:31.877498Z","caller":"traceutil/trace.go:171","msg":"trace[50058820] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:30194; }","duration":"137.545198ms","start":"2024-10-23T04:28:31.739926Z","end":"2024-10-23T04:28:31.877471Z","steps":["trace[50058820] 'range keys from in-memory index tree'  (duration: 137.240871ms)"],"step_count":1}
{"level":"info","ts":"2024-10-23T04:29:09.349513Z","caller":"traceutil/trace.go:171","msg":"trace[2059427670] linearizableReadLoop","detail":"{readStateIndex:36529; appliedIndex:36528; }","duration":"320.950043ms","start":"2024-10-23T04:29:09.028535Z","end":"2024-10-23T04:29:09.349485Z","steps":["trace[2059427670] 'read index received'  (duration: 320.691022ms)","trace[2059427670] 'applied index is now lower than readState.Index'  (duration: 258.021µs)"],"step_count":2}
{"level":"warn","ts":"2024-10-23T04:29:09.349758Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"321.197563ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-10-23T04:29:09.349808Z","caller":"traceutil/trace.go:171","msg":"trace[1223185072] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:30231; }","duration":"321.264968ms","start":"2024-10-23T04:29:09.028527Z","end":"2024-10-23T04:29:09.349792Z","steps":["trace[1223185072] 'agreement among raft nodes before linearized reading'  (duration: 321.064452ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:29:09.349855Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T04:29:09.028462Z","time spent":"321.379579ms","remote":"127.0.0.1:33754","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-10-23T04:29:09.352664Z","caller":"traceutil/trace.go:171","msg":"trace[1150955198] transaction","detail":"{read_only:false; response_revision:30231; number_of_response:1; }","duration":"359.552811ms","start":"2024-10-23T04:29:08.993075Z","end":"2024-10-23T04:29:09.352628Z","steps":["trace[1150955198] 'process raft request'  (duration: 356.217637ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:29:09.370862Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T04:29:08.993044Z","time spent":"359.697423ms","remote":"127.0.0.1:33872","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":537,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube-m02\" mod_revision:30220 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube-m02\" value_size:484 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube-m02\" > >"}
{"level":"warn","ts":"2024-10-23T04:29:18.985702Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"309.14995ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 ","response":"range_response_count:122 size:90424"}
{"level":"info","ts":"2024-10-23T04:29:18.985790Z","caller":"traceutil/trace.go:171","msg":"trace[413422811] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:122; response_revision:30240; }","duration":"309.26866ms","start":"2024-10-23T04:29:18.676504Z","end":"2024-10-23T04:29:18.985772Z","steps":["trace[413422811] 'range keys from bolt db'  (duration: 308.970334ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-23T04:29:18.985854Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-23T04:29:18.676450Z","time spent":"309.38247ms","remote":"127.0.0.1:33626","response type":"/etcdserverpb.KV/Range","request count":0,"request size":41,"response count":122,"response size":90447,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 "}


==> kernel <==
 04:29:20 up 26 min,  0 users,  load average: 8.01, 4.12, 2.95
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [4594cae8d2e8] <==
I1023 04:12:48.381036       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1023 04:12:48.381365       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1023 04:12:49.509996       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1023 04:12:49.510488       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1023 04:12:49.511673       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1023 04:12:49.513077       1 secure_serving.go:213] Serving securely on [::]:8443
I1023 04:12:49.513171       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1023 04:12:49.514284       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1023 04:12:49.518686       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1023 04:12:49.519436       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1023 04:12:49.524582       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1023 04:12:49.524857       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1023 04:12:49.524975       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1023 04:12:49.525227       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1023 04:12:49.525242       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1023 04:12:49.531476       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1023 04:12:49.534493       1 aggregator.go:169] waiting for initial CRD sync...
I1023 04:12:49.534686       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1023 04:12:49.536031       1 local_available_controller.go:156] Starting LocalAvailability controller
I1023 04:12:49.536070       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1023 04:12:49.536254       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1023 04:12:49.539259       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1023 04:12:49.539285       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1023 04:12:49.568057       1 controller.go:119] Starting legacy_token_tracking_controller
I1023 04:12:49.568638       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1023 04:12:49.569192       1 controller.go:78] Starting OpenAPI AggregationController
I1023 04:12:49.571004       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1023 04:12:49.581367       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1023 04:12:49.575468       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1023 04:12:49.576330       1 controller.go:142] Starting OpenAPI controller
I1023 04:12:49.576424       1 controller.go:90] Starting OpenAPI V3 controller
I1023 04:12:49.576452       1 naming_controller.go:294] Starting NamingConditionController
I1023 04:12:49.576508       1 establishing_controller.go:81] Starting EstablishingController
I1023 04:12:49.576663       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1023 04:12:49.576684       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1023 04:12:49.576699       1 crd_finalizer.go:269] Starting CRDFinalizer
I1023 04:12:49.697917       1 shared_informer.go:320] Caches are synced for configmaps
I1023 04:12:49.698710       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1023 04:12:49.728275       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1023 04:12:49.731651       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1023 04:12:49.738141       1 cache.go:39] Caches are synced for LocalAvailability controller
I1023 04:12:49.738731       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1023 04:12:49.738766       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1023 04:12:49.739791       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1023 04:12:49.739847       1 aggregator.go:171] initial CRD sync complete...
I1023 04:12:49.739859       1 autoregister_controller.go:144] Starting autoregister controller
I1023 04:12:49.739870       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1023 04:12:49.739879       1 cache.go:39] Caches are synced for autoregister controller
I1023 04:12:49.742627       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1023 04:12:49.744793       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1023 04:12:49.745204       1 policy_source.go:224] refreshing policies
I1023 04:12:49.769642       1 shared_informer.go:320] Caches are synced for node_authorizer
E1023 04:12:49.798024       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1023 04:12:49.835746       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1023 04:12:50.559714       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1023 04:12:58.909607       1 controller.go:615] quota admission added evaluator for: endpoints
I1023 04:12:58.912191       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E1023 04:15:02.291249       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 04:15:03.256979       1 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
E1023 04:15:03.339770       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": the object has been modified; please apply your changes to the latest version and try again"


==> kube-apiserver [ed7551dc2c4b] <==
E1023 02:37:11.352348       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:37:11.354259       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.081440864s" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E1023 02:37:24.771407       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:37:24.771536       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 58.617µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1023 02:37:24.772942       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:37:24.823825       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:37:24.967911       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="196.523599ms" method="POST" path="/api/v1/namespaces/default/events" result=null
E1023 02:37:31.933752       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:37:32.281463       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:39:30.504145       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:39:30.915278       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:39:31.958848       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:39:32.307000       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:39:32.308359       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:39:32.775816       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: etcdserver: request timed out" logger="UnhandledError"
E1023 02:39:32.964359       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.003699171s, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E1023 02:39:32.982310       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:39:33.915286       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="14.102µs" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m03" result=null
E1023 02:39:34.553779       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.3370732s, panicked: false, err: etcdserver: request timed out, panic-reason: <nil>" logger="UnhandledError"
E1023 02:39:34.554624       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:39:34.555376       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="5.601µs" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E1023 02:39:34.555700       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:39:34.559723       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:39:34.563026       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.236793995s" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02" result=null
E1023 02:39:35.237800       1 wrap.go:53] "Timeout or abort while handling" logger="UnhandledError" method="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m03?timeout=10s" auditID="217b55e6-131a-4619-b18f-53724620fb68"
E1023 02:39:43.950730       1 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
E1023 02:39:44.313998       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:39:44.001551       1 compact.go:124] etcd: endpoint ([https://127.0.0.1:2379]) compact failed: etcdserver: request timed out
E1023 02:39:44.440290       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:39:44.711869       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:39:44.712812       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 398.701927ms, panicked: false, err: etcdserver: request timed out, panic-reason: <nil>" logger="UnhandledError"
E1023 02:39:46.297803       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:39:46.685745       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="201.881207ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E1023 02:39:47.923595       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": the object has been modified; please apply your changes to the latest version and try again"
E1023 02:43:57.786460       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:43:58.904107       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 15.12µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1023 02:43:58.908365       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:43:58.908377       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="12.016µs" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m03" result=null
E1023 02:43:59.064115       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 23.33µs, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E1023 02:43:57.067987       1 wrap.go:53] "Timeout or abort while handling" logger="UnhandledError" method="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m03?timeout=10s" auditID="1e90f6f0-ae6c-4e32-a10b-8600e35f7b87"
E1023 02:43:59.142333       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.142459       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 72.295µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1023 02:43:59.143422       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.145182       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.145447       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.922410165s, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E1023 02:43:59.145978       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.146048       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.150309       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.150534       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="8.035839ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02" result=null
E1023 02:43:59.150642       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:43:59.151577       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="90.585013ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E1023 02:43:59.151752       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.902604488s" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E1023 02:43:59.873325       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E1023 02:43:59.892601       1 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
E1023 02:44:02.176803       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": the object has been modified; please apply your changes to the latest version and try again"
E1023 02:44:06.139860       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:44:06.141364       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1023 02:44:06.153741       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1023 02:44:06.185231       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 43.793458ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1023 02:44:06.185525       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="57.416342ms" method="POST" path="/api/v1/namespaces/kube-system/events" result=null


==> kube-controller-manager [94b5f9851dde] <==
	k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001e17180?})
		k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
	panic({0x2d99000?, 0x5480790?})
		runtime/panic.go:770 +0x132
	k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc0007e6820, 0xc001c3cc88, 0xc00228c008)
		k8s.io/cloud-provider/controllers/service/controller.go:562 +0x728
	k8s.io/cloud-provider/controllers/service.New.func2({0x326c8e0?, 0xc001c3cc88?}, {0x326c8e0, 0xc00228c008?})
		k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
	k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
		k8s.io/client-go/tools/cache/controller.go:253
	k8s.io/client-go/tools/cache.(*processorListener).run.func1()
		k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
	k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
		k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
	k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc002121f70, {0x380e700, 0xc001cb34a0}, 0x1, 0xc001c8d260)
		k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
	k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001d58770, 0x3b9aca00, 0x0, 0x1, 0xc001c8d260)
		k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
	k8s.io/apimachinery/pkg/util/wait.Until(...)
		k8s.io/apimachinery/pkg/util/wait/backoff.go:161
	k8s.io/client-go/tools/cache.(*processorListener).run(0xc0009fde60)
		k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
	k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
		k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
	created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 886
		k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
 >
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x211c768]

goroutine 924 [running]:
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x38432a0, 0x5545b00}, {0x2d99000, 0x5480790}, {0x5545b00, 0x0, 0x43d945?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001e17180?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x2d99000?, 0x5480790?})
	runtime/panic.go:770 +0x132
k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc0007e6820, 0xc001c3cc88, 0xc00228c008)
	k8s.io/cloud-provider/controllers/service/controller.go:562 +0x728
k8s.io/cloud-provider/controllers/service.New.func2({0x326c8e0?, 0xc001c3cc88?}, {0x326c8e0, 0xc00228c008?})
	k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
	k8s.io/client-go/tools/cache/controller.go:253
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
	k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc001c63f70, {0x380e700, 0xc001cb34a0}, 0x1, 0xc001c8d260)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001d58770, 0x3b9aca00, 0x0, 0x1, 0xc001c8d260)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
k8s.io/apimachinery/pkg/util/wait.Until(...)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc0009fde60)
	k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
	k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 886
	k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73


==> kube-controller-manager [f5ac537168bd] <==
I1023 04:27:01.695604       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1023 04:27:01.700883       1 shared_informer.go:320] Caches are synced for expand
I1023 04:27:01.706853       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1023 04:27:01.729416       1 shared_informer.go:320] Caches are synced for service account
I1023 04:27:01.729756       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1023 04:27:01.770935       1 shared_informer.go:320] Caches are synced for TTL after finished
I1023 04:27:01.789265       1 shared_informer.go:320] Caches are synced for namespace
I1023 04:27:01.831351       1 shared_informer.go:320] Caches are synced for cronjob
I1023 04:27:02.099916       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1023 04:27:02.101058       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m02\" does not exist"
I1023 04:27:02.101175       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m03\" does not exist"
I1023 04:27:02.123324       1 shared_informer.go:320] Caches are synced for node
I1023 04:27:02.124033       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1023 04:27:02.125278       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1023 04:27:02.125317       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1023 04:27:02.125366       1 shared_informer.go:320] Caches are synced for cidrallocator
I1023 04:27:02.125967       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1023 04:27:02.131298       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube-m02"
I1023 04:27:02.131503       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube-m03"
I1023 04:27:02.173036       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1023 04:27:02.173531       1 shared_informer.go:320] Caches are synced for endpoint
I1023 04:27:02.173807       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1023 04:27:02.174120       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
I1023 04:27:02.174780       1 shared_informer.go:320] Caches are synced for daemon sets
I1023 04:27:02.183732       1 shared_informer.go:320] Caches are synced for TTL
I1023 04:27:02.184451       1 shared_informer.go:320] Caches are synced for taint
I1023 04:27:02.204104       1 shared_informer.go:320] Caches are synced for ephemeral
I1023 04:27:02.204200       1 shared_informer.go:320] Caches are synced for crt configmap
I1023 04:27:02.204894       1 shared_informer.go:320] Caches are synced for ReplicationController
I1023 04:27:02.215763       1 shared_informer.go:320] Caches are synced for GC
I1023 04:27:02.219491       1 shared_informer.go:320] Caches are synced for job
I1023 04:27:02.222203       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1023 04:27:02.222342       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1023 04:27:02.222417       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube-m02"
I1023 04:27:02.222454       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube-m03"
I1023 04:27:02.222505       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1023 04:27:02.222889       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-c-6bc8b97877" duration="87.908µs"
I1023 04:27:02.227842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.85723ms"
I1023 04:27:02.228060       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="146.113µs"
I1023 04:27:02.228193       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="61.906µs"
I1023 04:27:02.240017       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1023 04:27:02.241866       1 shared_informer.go:320] Caches are synced for stateful set
I1023 04:27:02.242036       1 shared_informer.go:320] Caches are synced for resource quota
I1023 04:27:02.242132       1 shared_informer.go:320] Caches are synced for HPA
I1023 04:27:02.242177       1 shared_informer.go:320] Caches are synced for disruption
I1023 04:27:02.242212       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1023 04:27:02.245482       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1023 04:27:02.229264       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1023 04:27:02.250755       1 shared_informer.go:320] Caches are synced for persistent volume
I1023 04:27:02.255972       1 shared_informer.go:320] Caches are synced for deployment
I1023 04:27:02.258639       1 shared_informer.go:320] Caches are synced for PVC protection
I1023 04:27:02.258730       1 shared_informer.go:320] Caches are synced for resource quota
I1023 04:27:02.258789       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1023 04:27:02.274528       1 shared_informer.go:320] Caches are synced for attach detach
I1023 04:27:02.336569       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1023 04:27:02.545598       1 shared_informer.go:320] Caches are synced for garbage collector
I1023 04:27:02.646638       1 shared_informer.go:320] Caches are synced for garbage collector
I1023 04:27:02.646739       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1023 04:28:09.503710       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1023 04:28:59.097776       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube-m02"


==> kube-proxy [6f4e91648d66] <==
E1023 04:13:08.828236       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1023 04:13:08.846938       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1023 04:13:08.988327       1 server_linux.go:66] "Using iptables proxy"
I1023 04:13:10.115730       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E1023 04:13:10.123844       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1023 04:13:10.779078       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1023 04:13:10.779684       1 server_linux.go:169] "Using iptables Proxier"
I1023 04:13:10.880846       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1023 04:13:10.904928       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1023 04:13:10.947004       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1023 04:13:10.947280       1 server.go:483] "Version info" version="v1.31.0"
I1023 04:13:10.947319       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1023 04:13:10.989462       1 config.go:197] "Starting service config controller"
I1023 04:13:10.989552       1 shared_informer.go:313] Waiting for caches to sync for service config
I1023 04:13:10.989822       1 config.go:104] "Starting endpoint slice config controller"
I1023 04:13:10.989838       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1023 04:13:10.990446       1 config.go:326] "Starting node config controller"
I1023 04:13:10.990465       1 shared_informer.go:313] Waiting for caches to sync for node config
I1023 04:13:11.127344       1 shared_informer.go:320] Caches are synced for node config
I1023 04:13:11.127836       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1023 04:13:11.190572       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [bb5f228d5ac8] <==
E1023 02:19:06.270481       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1023 02:19:07.475348       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1023 02:19:15.406512       1 server_linux.go:66] "Using iptables proxy"
E1023 02:19:48.113301       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": net/http: TLS handshake timeout"
I1023 02:19:52.323765       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E1023 02:19:52.726630       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1023 02:19:58.074456       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1023 02:19:58.074838       1 server_linux.go:169] "Using iptables Proxier"
I1023 02:19:58.216541       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1023 02:20:00.251574       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1023 02:20:00.705561       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1023 02:20:01.016809       1 server.go:483] "Version info" version="v1.31.0"
I1023 02:20:01.017004       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1023 02:20:02.485368       1 config.go:197] "Starting service config controller"
I1023 02:20:02.491277       1 config.go:104] "Starting endpoint slice config controller"
I1023 02:20:02.676629       1 shared_informer.go:313] Waiting for caches to sync for service config
I1023 02:20:02.676799       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1023 02:20:02.493235       1 config.go:326] "Starting node config controller"
I1023 02:20:02.676955       1 shared_informer.go:313] Waiting for caches to sync for node config
I1023 02:20:02.990309       1 shared_informer.go:320] Caches are synced for node config
I1023 02:20:03.413716       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1023 02:20:03.501730       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [29b960ba2a66] <==
W1023 02:19:40.627157       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.627245       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.651398       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.651573       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.806540       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.806620       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.937368       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.937450       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.937711       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.937766       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.940730       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.940807       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.941153       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.941221       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:40.941530       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:40.941602       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:41.089753       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:41.089826       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:41.159553       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:41.159631       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:41.232299       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:41.232383       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:41.357727       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:41.357815       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:41.604417       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:41.604567       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:41.604954       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1023 02:19:41.638076       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1023 02:19:44.916778       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1023 02:19:44.916961       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1023 02:19:44.917161       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1023 02:19:44.917870       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1023 02:19:44.919711       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1023 02:19:44.919795       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1023 02:19:44.930211       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1023 02:19:44.930308       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1023 02:19:44.953170       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1023 02:19:44.970543       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1023 02:19:44.970751       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1023 02:19:44.971570       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1023 02:19:44.971811       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1023 02:19:44.971895       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1023 02:19:44.972306       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1023 02:19:45.069161       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1023 02:19:45.308394       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1023 02:19:45.308510       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1023 02:19:45.308779       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1023 02:19:45.342980       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1023 02:19:45.343056       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1023 02:19:45.343212       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1023 02:19:45.343243       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1023 02:19:45.343419       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1023 02:19:45.343457       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1023 02:19:45.343611       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1023 02:19:45.343646       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1023 02:19:45.343799       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1023 02:19:45.620569       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1023 02:19:45.620654       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1023 02:19:50.759081       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W1023 02:26:45.432371       1 reflector.go:484] runtime/asm_amd64.s:1695: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding


==> kube-scheduler [fa6f4e5e21ee] <==
I1023 04:12:46.133475       1 serving.go:386] Generated self-signed cert in-memory
W1023 04:12:49.715770       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1023 04:12:49.716291       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1023 04:12:49.716687       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1023 04:12:49.717083       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1023 04:12:49.778137       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1023 04:12:49.778193       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1023 04:12:49.789995       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1023 04:12:49.791635       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1023 04:12:49.793440       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1023 04:12:49.794961       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1023 04:12:49.896258       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Oct 23 04:21:18 minikube kubelet[1607]: E1023 04:21:18.647967    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:21:30 minikube kubelet[1607]: E1023 04:21:30.647858    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:21:43 minikube kubelet[1607]: E1023 04:21:43.646255    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:21:55 minikube kubelet[1607]: E1023 04:21:55.649829    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:22:09 minikube kubelet[1607]: E1023 04:22:09.656978    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:22:22 minikube kubelet[1607]: E1023 04:22:22.643429    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:22:36 minikube kubelet[1607]: E1023 04:22:36.643460    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:22:50 minikube kubelet[1607]: E1023 04:22:50.641454    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:23:05 minikube kubelet[1607]: E1023 04:23:05.640548    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:23:17 minikube kubelet[1607]: E1023 04:23:17.638743    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:23:28 minikube kubelet[1607]: E1023 04:23:28.637899    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:23:39 minikube kubelet[1607]: E1023 04:23:39.635594    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:23:50 minikube kubelet[1607]: E1023 04:23:50.634871    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:24:05 minikube kubelet[1607]: E1023 04:24:05.617195    1607 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for melissaocoro/servicioc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/servicioc:1.0"
Oct 23 04:24:05 minikube kubelet[1607]: E1023 04:24:05.617316    1607 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for melissaocoro/servicioc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/servicioc:1.0"
Oct 23 04:24:05 minikube kubelet[1607]: E1023 04:24:05.617544    1607 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-c,Image:melissaocoro/servicioc:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3007,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xps2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-c-6bc8b97877-z5jp9_default(b30f7cf9-d4bd-497a-ad82-d2118c717e59): ErrImagePull: Error response from daemon: pull access denied for melissaocoro/servicioc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 23 04:24:05 minikube kubelet[1607]: E1023 04:24:05.618923    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ErrImagePull: \"Error response from daemon: pull access denied for melissaocoro/servicioc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:24:20 minikube kubelet[1607]: E1023 04:24:20.632500    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:24:32 minikube kubelet[1607]: E1023 04:24:32.638797    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:24:44 minikube kubelet[1607]: E1023 04:24:44.630808    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:24:57 minikube kubelet[1607]: E1023 04:24:57.631063    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:25:09 minikube kubelet[1607]: E1023 04:25:09.627985    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:25:20 minikube kubelet[1607]: E1023 04:25:20.628660    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:25:34 minikube kubelet[1607]: E1023 04:25:34.627317    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:25:47 minikube kubelet[1607]: I1023 04:25:47.139336    1607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-76q7f\" (UniqueName: \"kubernetes.io/projected/ed729d40-cb53-46cf-86ae-ef2f30e1720f-kube-api-access-76q7f\") pod \"service-c-pod\" (UID: \"ed729d40-cb53-46cf-86ae-ef2f30e1720f\") " pod="default/service-c-pod"
Oct 23 04:25:48 minikube kubelet[1607]: I1023 04:25:48.623247    1607 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9a8c12438eb0802b8cd08578ff579fc198e00a19f035487afdb803d931bf6cf3"
Oct 23 04:25:49 minikube kubelet[1607]: E1023 04:25:49.627766    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:25:50 minikube kubelet[1607]: E1023 04:25:50.923225    1607 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/service-c:1.0"
Oct 23 04:25:50 minikube kubelet[1607]: E1023 04:25:50.923578    1607 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/service-c:1.0"
Oct 23 04:25:50 minikube kubelet[1607]: E1023 04:25:50.923769    1607 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-c-container,Image:melissaocoro/service-c:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76q7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-c-pod_default(ed729d40-cb53-46cf-86ae-ef2f30e1720f): ErrImagePull: Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 23 04:25:50 minikube kubelet[1607]: E1023 04:25:50.929240    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ErrImagePull: \"Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:25:51 minikube kubelet[1607]: E1023 04:25:51.699951    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/service-c:1.0\\\"\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:26:03 minikube kubelet[1607]: E1023 04:26:03.623844    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:26:06 minikube kubelet[1607]: E1023 04:26:06.370803    1607 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/service-c:1.0"
Oct 23 04:26:06 minikube kubelet[1607]: E1023 04:26:06.370926    1607 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/service-c:1.0"
Oct 23 04:26:06 minikube kubelet[1607]: E1023 04:26:06.371210    1607 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-c-container,Image:melissaocoro/service-c:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76q7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-c-pod_default(ed729d40-cb53-46cf-86ae-ef2f30e1720f): ErrImagePull: Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 23 04:26:06 minikube kubelet[1607]: E1023 04:26:06.372968    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ErrImagePull: \"Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:26:18 minikube kubelet[1607]: E1023 04:26:18.621798    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:26:20 minikube kubelet[1607]: E1023 04:26:20.622495    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/service-c:1.0\\\"\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:26:33 minikube kubelet[1607]: E1023 04:26:33.656249    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:26:34 minikube kubelet[1607]: E1023 04:26:34.453861    1607 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/service-c:1.0"
Oct 23 04:26:34 minikube kubelet[1607]: E1023 04:26:34.453978    1607 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="melissaocoro/service-c:1.0"
Oct 23 04:26:34 minikube kubelet[1607]: E1023 04:26:34.454165    1607 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-c-container,Image:melissaocoro/service-c:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76q7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-c-pod_default(ed729d40-cb53-46cf-86ae-ef2f30e1720f): ErrImagePull: Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Oct 23 04:26:34 minikube kubelet[1607]: E1023 04:26:34.457109    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ErrImagePull: \"Error response from daemon: pull access denied for melissaocoro/service-c, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:26:48 minikube kubelet[1607]: E1023 04:26:48.620109    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:26:49 minikube kubelet[1607]: E1023 04:26:49.620318    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/service-c:1.0\\\"\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:26:56 minikube kubelet[1607]: I1023 04:26:56.433033    1607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8skqn\" (UniqueName: \"kubernetes.io/projected/fa8ec3f2-fac6-4c0c-9064-0160ae8f0ea9-kube-api-access-8skqn\") pod \"service-pod\" (UID: \"fa8ec3f2-fac6-4c0c-9064-0160ae8f0ea9\") " pod="default/service-pod"
Oct 23 04:26:58 minikube kubelet[1607]: I1023 04:26:58.025183    1607 scope.go:117] "RemoveContainer" containerID="94b5f9851dde65ff5f38429b198f879db2532e5f22b3f7e99cc897b7016bcf08"
Oct 23 04:26:58 minikube kubelet[1607]: I1023 04:26:58.027103    1607 scope.go:117] "RemoveContainer" containerID="f811571563caf8c6d584410378213756d9b53cf118dbacbc3b0db48f88664a9e"
Oct 23 04:27:00 minikube kubelet[1607]: E1023 04:27:00.623124    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:27:04 minikube kubelet[1607]: E1023 04:27:04.621555    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c-container\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/service-c:1.0\\\"\"" pod="default/service-c-pod" podUID="ed729d40-cb53-46cf-86ae-ef2f30e1720f"
Oct 23 04:27:15 minikube kubelet[1607]: E1023 04:27:15.617821    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:27:26 minikube kubelet[1607]: E1023 04:27:26.617339    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:27:39 minikube kubelet[1607]: E1023 04:27:39.751710    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:27:52 minikube kubelet[1607]: E1023 04:27:52.618309    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:28:03 minikube kubelet[1607]: E1023 04:28:03.624499    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:28:15 minikube kubelet[1607]: E1023 04:28:15.632210    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:28:27 minikube kubelet[1607]: E1023 04:28:27.622592    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:28:42 minikube kubelet[1607]: E1023 04:28:42.656886    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"
Oct 23 04:28:57 minikube kubelet[1607]: E1023 04:28:57.655612    1607 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-c\" with ImagePullBackOff: \"Back-off pulling image \\\"melissaocoro/servicioc:1.0\\\"\"" pod="default/service-c-6bc8b97877-z5jp9" podUID="b30f7cf9-d4bd-497a-ad82-d2118c717e59"

